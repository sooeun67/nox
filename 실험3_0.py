# -*- coding: utf-8 -*-
"""á„‰á…µá†¯á„’á…¥á†·3-0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-qlf04MglI8nGJvVgzUI1cBN08gTsxy

# (í•„ìš” ì‹œ ì‚¬ìš©) Colab í™˜ê²½ ì„¤ì •

## matplitlib ê·¸ë¦¼ì— í•œê¸€ í‘œì‹œí•˜ê¸° - colabì—ì„œ í•œê¸€ í‘œì‹œ ì•ˆë¼ì„œ ì–´ì©” ìˆ˜ ì—†ì´ ì‚¬ìš©ì¤‘...
"""

# 1. ë‚˜ëˆ”ê¸€ê¼´ ì„¤ì¹˜
!apt-get -qq -y install fonts-nanum

# 2. ëŸ°íƒ€ì„ ìºì‹œ ì§€ìš°ê¸° ë° ì¬ë¡œë“œë¥¼ ìœ„í•œ matplotlib ìºì‹œ ì‚­ì œ
import matplotlib
import shutil
import os

matplotlib_cache_dir = matplotlib.get_cachedir()
if os.path.exists(matplotlib_cache_dir):
    shutil.rmtree(matplotlib_cache_dir)

# 3. ëŸ°íƒ€ì„ ì¬ì‹œì‘ (ì¤‘ìš”)
os.kill(os.getpid(), 9)

"""## Google Drive ë§ˆìš´íŠ¸"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/test_data_for_inference.csv")

sample = df[:200]
sample.to_csv("sample.csv")





"""# í™˜ê²½ ì„¤ì •"""

!pip install polars pyarrow openpyxl

import os
import ast
import numpy as np
import pandas as pd
import polars as pl
import seaborn as sns

from functools import reduce
from datetime import date, datetime, timedelta
import datetime as dt
from typing import Dict, List, Tuple

# 1. ì „ì—­ ì„¤ì • ë³€ê²½ (ë…¸íŠ¸ë¶ ì „ì²´ì— ì ìš©)
pl.Config.set_tbl_rows(30)  # ê¸°ë³¸ê°’ì€ ë³´í†µ 10
pl.Config.set_tbl_cols(20)   # ì»¬ëŸ¼ ê°œìˆ˜ë„ ì¡°ì • ê°€ëŠ¥

import warnings
warnings.filterwarnings("ignore")

# ì¬ì‹œì‘ í›„: í°íŠ¸ ì„¤ì •
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False

dir_save = os.path.join(
    '/content/drive/MyDrive/SKEP/0.Colab Notebooks/2.Modeling/[á„†á…µá†®á„‹á…³á†·_á„‹á…¨á„á…³á†¨á„†á…©á„ƒá…¦á†¯_0801]',
    'ì‹¤í—˜8'
)
os.makedirs(dir_save, exist_ok=True)

"""## ë©”íƒ€ë°ì´í„°"""

meta_dat = pd.read_excel('/content/drive/MyDrive/SKEP/0.Colab Notebooks/2.Modeling/[á„†á…µá†®á„‹á…³á†·_á„‹á…¨á„á…³á†¨á„†á…©á„ƒá…¦á†¯_0801]/á„†á…¦á„á…¡á„ƒá…¦á„‹á…µá„á…¥_v2_250804.xlsx', sheet_name='ìš”ì†Œìˆ˜_RAW_DATA')
des = (
    meta_dat[['DATAFIELD_lowercased','Description','ê³„ì¸¡ í•­ëª© ','SOURCE']]
    .dropna(subset=['DATAFIELD_lowercased', 'Description'], how='all')
    .reset_index(drop=True)
)
des

"""## ì´ìƒì¹˜/ê²°ì¸¡ì¹˜ ì „ì²˜ë¦¬ í›„ ë°ì´í„°"""

# ê²½ë¡œ ì„¤ì •
# path_parquet_data = '/content/drive/MyDrive/SKEP/250708_5sMean/cleaned_240411_250704.parquet'
path_parquet_data = '/content/drive/MyDrive/SKEP/250725_SRS1_á„‰á…µá†«á„€á…²á„‰á…®á„Œá…µá†¸/cleaned_240411_250724.parquet'

# ì„ íƒ ì»¬ëŸ¼
col_datetime   = "_time_gateway"
col_nox_tms_af = 'nox_value'
col_nox_tms_bf = 'icf_tms_nox_a'
col_pump_hz = 'snr_pmp_uw_s_1'
cols_x_raw = [

     'bft_eo_fg_t'
    ,'br1_eo_fg_t'
    ,'br1_eo_o2_a'
    ,'br1_eo_st_t'
    ,'dr1_eq_bw_c'
    ,'icf_ccs_fg_t_1'
    ,'icf_cra_wt_k'
    ,'icf_ff1_ar_f_1'
    ,'icf_ff1_ss_s_1'
    ,'icf_ff1_ss_s_2'
    ,'icf_ff2_ss_s_1'
    ,'icf_idf_ss_s_1'
    ,'icf_scs_fg_t_1'
    ,'sdr_htr_fg_t'
]

# âœ… datetime ì¡°ê±´ í•„í„° ì ìš© (ë‚ ì§œë¥¼ ëª…ì‹œì ìœ¼ë¡œ datetimeìœ¼ë¡œ)
df = (
    pl.scan_parquet(path_parquet_data)
    .filter(pl.col("_time_gateway") >= pl.datetime(2025, 5, 22))
    # .filter(pl.col("_time_gateway") >= pl.datetime(2025, 4, 1))
    .select([col_datetime, col_nox_tms_af, col_nox_tms_bf, col_pump_hz] + cols_x_raw)
    .collect()
    .to_pandas()
)

df.shape

"""# íŒŒìƒë³€ìˆ˜ ìƒì„±

## íê¸°ë¬¼ íˆ¬ì…
"""

# íê¸°ë¬¼ íˆ¬ì… ê´€ë ¨ Feature ìƒì„±
col_trash_drop = 'trash_drop'
col_trash_drop_count_30min = 'trash_drop_count_30min'

### âœ… íê¸°ë¬¼ íˆ¬ì… ì—¬ë¶€ ë° ëˆ„ì  íšŸìˆ˜ ###

# df = df_filtered.copy()
df = df.set_index('_time_gateway')

window_size_sec = 10
diff_tolerance = -10
df[col_trash_drop] = (
    df['icf_cra_wt_k'].bfill().rolling(window_size_sec).max().diff() < diff_tolerance
).astype(int)

df[col_trash_drop_count_30min] = (
    df[col_trash_drop].rolling("30min").sum().fillna(0)
)

df = df.reset_index()
# df.drop(columns=col_trash_waiting_time, inplace=True)

"""## ìš”ì•½í†µê³„ëŸ‰"""

def generate_interval_summary_features_inplace_time(
    df,
    datetime_col,
    columns,
    interval_seconds=[int(60*x) for x in [1, 3, 5, 10, 30]]
):
    df.sort_values(datetime_col, inplace=True)

    new_columns = []

    for col in columns:
        print(col)
        for sec in interval_seconds:
            window = pd.Timedelta(seconds=sec)

            # í‰ê· /í‘œì¤€í¸ì°¨
            mean_col = f"{col}_mean_{sec}s"
            std_col = f"{col}_std_{sec}s"
            df[mean_col] = df.rolling(window=window, on=datetime_col)[col].mean()
            df[std_col] = df.rolling(window=window, on=datetime_col)[col].std()
            new_columns.extend([mean_col, std_col])

            # ì‹œì‘ê°’
            df_prev = df[[datetime_col, col]]
            df_prev_shifted = df_prev.copy()
            df_prev_shifted[datetime_col] += pd.Timedelta(seconds=sec)
            df_prev_shifted.rename(columns={col: "_start_tmp"}, inplace=True)

            df_matched = pd.merge_asof(
                df, df_prev_shifted,
                on=datetime_col,
                direction="backward",
                tolerance=pd.Timedelta(seconds=0)
            )

            start_val = df_matched["_start_tmp"].values
            end_val = df[col].values

            # ë³€í™”ìœ¨/ë³€í™”ëŸ‰
            mean_rate_col = f"{col}_mean_rate_change_{sec}s"
            range_change_col = f"{col}_range_change_{sec}s"
            df[mean_rate_col] = (end_val - start_val) / start_val
            df[range_change_col] = end_val - start_val
            new_columns.extend([mean_rate_col, range_change_col])

            # âœ… ì´ˆë‹¹ ë³€í™”ìœ¨ ê¸°ë°˜ momentum
            df["_val_diff"] = df[col].diff()
            df["_time_diff"] = df[datetime_col].diff().dt.total_seconds()
            df["_rate_per_sec"] = df["_val_diff"] / df["_time_diff"]

            momentum_up_col = f"{col}_momentum_max_up_{sec}s"
            momentum_down_col = f"{col}_momentum_max_down_{sec}s"
            df[momentum_up_col] = df.rolling(window=window, on=datetime_col)["_rate_per_sec"].max()
            df[momentum_down_col] = df.rolling(window=window, on=datetime_col)["_rate_per_sec"].min()
            new_columns.extend([momentum_up_col, momentum_down_col])

            # ì‹œì‘ê°’ ëŒ€ë¹„ ìµœëŒ€ ì¦ê°€/ê°ì†ŒëŸ‰
            rolling_window = df.rolling(window=window, on=datetime_col)[col]
            max_inc_col = f"{col}_max_increase_from_start_{sec}s"
            max_dec_col = f"{col}_max_decrease_from_start_{sec}s"
            df[max_inc_col] = rolling_window.max() - start_val
            df[max_dec_col] = rolling_window.min() - start_val
            new_columns.extend([max_inc_col, max_dec_col])

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df.drop(columns=["_val_diff", "_time_diff", "_rate_per_sec"], inplace=True, errors="ignore")

    return new_columns

cols_x_original = [

     'bft_eo_fg_t'
    ,'br1_eo_fg_t'
    ,'br1_eo_o2_a'
    ,'br1_eo_st_t'
    ,'dr1_eq_bw_c'
    ,'icf_ccs_fg_t_1'
    ,'icf_cra_wt_k'
    ,'icf_ff1_ar_f_1'
    ,'icf_ff1_ss_s_1'
    ,'icf_ff1_ss_s_2'
    ,'icf_ff2_ss_s_1'
    ,'icf_idf_ss_s_1'
    ,'icf_scs_fg_t_1'
    ,'icf_tms_nox_a'
    ,'sdr_htr_fg_t'

    ,'trash_drop'
    ,'trash_drop_count_30min'
]

len(cols_x_original)

"""### (Feature ì‚¬ìš© X) ìš”ì†Œìˆ˜ Pump Hz
- ì˜ˆì¸¡ ëª¨í˜•ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ì¶”í›„ ì œì–´ EDA ë° ëª¨ë¸ë§ ì—¼ë‘ì— ë‘ê³  ìƒì„±
"""

cols_hz_stat = generate_interval_summary_features_inplace_time(df, col_datetime, [col_pump_hz])

"""### ìš”ì•½í†µê³„ëŸ‰ Feature
- ì›ë³¸ 15ê°œ + íê¸°ë¬¼ íˆ¬ì… íŒŒìƒë³€ìˆ˜ 2ê°œ
"""

# inplaceë¡œ ìƒì„± í›„ column ëª©ë¡ ë°˜í™˜
cols_x_stat = generate_interval_summary_features_inplace_time(df, col_datetime, cols_x_original)

len(cols_x_stat)

df[cols_x_stat + cols_hz_stat].head()

"""## is_spike
- (ì°¸ê³ ) near_spike: ë¶„ì„ ì œì™¸
"""

def mark_nox_spikes(
    df,
    time_col="_time_gateway",
    nox_col="nox_value",
    window_time_sec=60,         # ğŸ”¹ 1ë¶„ Rolling window (ì´ˆ ë‹¨ìœ„)
    spike_range_threshold=8,
    spike_std_threshold=6,
    spike_window_sec=120        # ğŸ”¹ ìŠ¤íŒŒì´í¬ ì£¼ë³€ Â±ì´ˆ ë‹¨ìœ„
):
    """
    âœ… NOx ê¸‰ë“±ë½(spike) ë° ì£¼ë³€ êµ¬ê°„ ë§ˆí‚¹ (ë¶ˆê·œì¹™ timestamp ëŒ€ì‘, ì‹œê°„ ê¸°ì¤€)
    - dfë¥¼ ì§ì ‘ ìˆ˜ì • (in-place), return ì—†ìŒ

    Parameters:
    - df : pd.DataFrame (ì›ë³¸ì— ì»¬ëŸ¼ ì¶”ê°€ë¨)
    - time_col : str (datetime ì»¬ëŸ¼ëª…)
    - nox_col : str (NOx ê°’ ì»¬ëŸ¼ëª…)
    - window_time_sec : int (rolling ìœˆë„ìš° ê¸¸ì´, ì´ˆ)
    - spike_range_threshold : float (ê¸‰ë“±ë½ range ê¸°ì¤€)
    - spike_std_threshold : float (ê¸‰ë“±ë½ std ê¸°ì¤€)
    - spike_window_sec : int (ìŠ¤íŒŒì´í¬ ì£¼ë³€ ì‹œê°„ ë²”ìœ„, ì´ˆ)
    """

    print("ğŸ“ˆ ê¸‰ë“±ë½ í”¼ì²˜ ìƒì„± ì¤‘...")

    # âœ… ì‹œê°„ ê¸°ë°˜ rolling window
    window_time = pd.Timedelta(seconds=window_time_sec)
    df["nox_range_1min"] = (
        df.rolling(window=window_time, on=time_col)[nox_col].max() -
        df.rolling(window=window_time, on=time_col)[nox_col].min()
    )
    df["nox_std_1min"] = df.rolling(window=window_time, on=time_col)[nox_col].std()

    # âœ… ê¸‰ë“±ë½ ì—¬ë¶€
    df["is_spike"] = (
        (df["nox_range_1min"] > spike_range_threshold) &
        (df["nox_std_1min"] < spike_std_threshold)
    ).astype(int)

    spike_count = df["is_spike"].sum()
    print(f"   âœ… ê¸‰ë“±ë½ êµ¬ê°„ íƒì§€: {spike_count}ê°œ ({spike_count/len(df)*100:.2f}%)")

    # âœ… ìŠ¤íŒŒì´í¬ ì£¼ë³€ ê³¼ê±°/ë¯¸ë˜ ë§ˆí‚¹
    print("ğŸ¯ ìŠ¤íŒŒì´í¬ ì£¼ë³€ êµ¬ê°„ ë§ˆí‚¹ ì¤‘...")
    mask_past = pd.Series(False, index=df.index)
    mask_future = pd.Series(False, index=df.index)

    spike_times = df.loc[df["is_spike"] == 1, time_col]

    return None

mark_nox_spikes(df, time_col="_time_gateway", nox_col="nox_value")

"""# target(2.5ë¶„ ë’¤ NOxê°’) ë° targetì— í•´ë‹¹í•˜ëŠ” weight ìƒì„±"""

def create_target_and_weights(
    df,
    time_col,
    nox_col,
    delta_sec=150
):
    print("ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

    # íƒ€ê²Ÿ ìƒì„±
    target_df = df[[time_col, nox_col]].copy()
    target_df[time_col] = target_df[time_col] - pd.Timedelta(seconds=delta_sec)
    target_df = target_df.rename(columns={nox_col: "target"})

    # âœ… merge í›„ df ì¬í• ë‹¹
    df = pd.merge(df, target_df, on=time_col, how="left")
    print('')
    print(f"   âœ… íƒ€ê²Ÿ ë§¤í•‘ ì™„ë£Œ (ê²°ì¸¡ íƒ€ê²Ÿ: {df['target'].isna().sum():,}ê°œ)")

    # ê°€ì¤‘ì¹˜ ì„¤ì •
    print("âš–ï¸ ê°€ì¤‘ì¹˜ ì„¤ì • ì¤‘...")
    def custom_weight(value):
        if pd.isna(value):
            return 1.0
        elif value < 20:
            return 1.0
        elif value < 30:
            return 1.5
        elif value < 40:
            return 3.0
        elif value >= 60:
            return 3.0
        else:
            return 2.0

    df["weights"] = df["target"].apply(custom_weight)

    weight_stats = df["weights"].value_counts().sort_index()
    print("   ê°€ì¤‘ì¹˜ ë¶„í¬:")
    for weight, count in weight_stats.items():
        print(f"     ê°€ì¤‘ì¹˜ {weight}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")

    return df

df = create_target_and_weights(df, time_col="_time_gateway", nox_col="nox_value", delta_sec=150)

df.head()







"""# (í•„ìš” ì‹œ ì‚¬ìš©) ì¤‘ê°„ ì •ë³´ ì €ì¥"""

parquet_path = os.path.join(dir_save, 'df.parquet')
meta_path = os.path.join(dir_save, 'df_category_meta.json')

import json

# === 1ï¸âƒ£ category column ì •ë³´ ìˆ˜ì§‘ ===
cat_info = {}
for col in df.columns:
    if df[col].dtype.name == 'category':
        cat_info[col] = list(df[col].cat.categories.astype(str))

# === 2ï¸âƒ£ DataFrame ì €ì¥ (category â†’ str ë³€í™˜) ===
df_for_save = df.apply(lambda x: x.astype(str) if x.dtype.name == 'category' else x)
df_for_save.to_parquet(parquet_path, index=False)

# === 3ï¸âƒ£ category ë©”íƒ€ë°ì´í„° ì €ì¥ ===
with open(meta_path, 'w', encoding='utf-8') as f:
    json.dump(cat_info, f, ensure_ascii=False, indent=2)

print(f"âœ… DataFrame ì €ì¥ ì™„ë£Œ: {parquet_path}")
print(f"âœ… Category ì •ë³´ ì €ì¥ ì™„ë£Œ: {meta_path}")

# # parquet ë¶ˆëŸ¬ì˜¤ê¸°
# df = pd.read_parquet(parquet_path)

# # category ë©”íƒ€ ë¶ˆëŸ¬ì˜¤ê¸°
# with open(meta_path, 'r', encoding='utf-8') as f:
#     cat_info = json.load(f)

# # category ë³µì›
# for col, cats in cat_info.items():
#     df[col] = pd.Categorical(df[col], categories=cats)

# print(df.dtypes)

### (ì„ì‹œ) ë°ì´í„° ì¤‘ê°„ë¶€í„° ë¶ˆëŸ¬ì˜¤ë©´ ì‚¬ìš© ###

# interval_seconds = [int(60*x) for x in [1, 3, 5, 10, 30]]
# cols_x_stat = []

# for col in cols_x_original:
#     for sec in interval_seconds:
#         cols_x_stat.extend([
#             f"{col}_mean_{sec}s",
#             f"{col}_std_{sec}s",
#             f"{col}_mean_rate_change_{sec}s",
#             f"{col}_range_change_{sec}s",
#             f"{col}_momentum_max_up_{sec}s",
#             f"{col}_momentum_max_down_{sec}s",
#             f"{col}_max_increase_from_start_{sec}s",
#             f"{col}_max_decrease_from_start_{sec}s"
#         ])







"""# ëª¨ë¸ë§ìš© ë°ì´í„°

## ë‚´ë¶€ ì˜¨ë„,ì¶œêµ¬ ì˜¨ë„ ë²”ìœ„ í™•ì¸
"""

df.shape

# 1. ì˜¨ë„ì™€ NOx ë²”ìœ„ ì„¤ì • (50ë„, 10ë‹¨ìœ„ êµ¬ê°„)
temp_bins = list(range(550, 1250, 50))  # ì˜ˆ: [1100, 1150, 1200]
nox_bins  = list(range(  0,  130, 10))  # ì˜ˆ: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['icf_ccs_fg_t_1'], bins=temp_bins, edgecolor='black', color='skyblue')
plt.xticks(temp_bins)
plt.xlabel('ë‚´ë¶€ ì˜¨ë„')
plt.ylabel('Count')
plt.title('ë‚´ë¶€ ì˜¨ë„ Histogram')
plt.tight_layout()
plt.show()

# 2. ê° ë³€ìˆ˜ binìœ¼ë¡œ ë‚˜ëˆ„ê¸°
df['temp_bin'] = pd.cut(df['icf_ccs_fg_t_1'], bins=temp_bins, right=False)
df[ 'nox_bin'] = pd.cut(df[     'nox_value'], bins= nox_bins, right=False)

# 3. 2D ê·¸ë£¹ë³„ ì¹´ìš´íŒ…
count_df = df.groupby(['nox_bin', 'temp_bin']).size().unstack(fill_value=0)
# 1. count_dfì˜ í–‰ ìˆœì„œë¥¼ ë’¤ì§‘ê¸° (ì—­ìˆœ)
count_df_reversed = count_df.iloc[::-1]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. ë‚´ë¶€ ì˜¨ë„')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

# 1. ì˜¨ë„ì™€ NOx ë²”ìœ„ ì„¤ì • (50ë„, 10ë‹¨ìœ„ êµ¬ê°„)
temp_bins = list(range(550, 1250, 50))  # ì˜ˆ: [1100, 1150, 1200]
nox_bins  = list(range(  0,  130, 10))  # ì˜ˆ: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['icf_scs_fg_t_1'], bins=temp_bins, edgecolor='black', color='skyblue')
plt.xticks(temp_bins)
plt.xlabel('ì¶œêµ¬ ì˜¨ë„')
plt.ylabel('Count')
plt.title('ì¶œêµ¬ ì˜¨ë„ Histogram')
plt.tight_layout()
plt.show()

# 2. ê° ë³€ìˆ˜ binìœ¼ë¡œ ë‚˜ëˆ„ê¸°
df['temp_bin'] = pd.cut(df['icf_scs_fg_t_1'], bins=temp_bins, right=False)
df[ 'nox_bin'] = pd.cut(df[     'nox_value'], bins= nox_bins, right=False)

# 3. 2D ê·¸ë£¹ë³„ ì¹´ìš´íŒ…
count_df = df.groupby(['nox_bin', 'temp_bin']).size().unstack(fill_value=0)
# 1. count_dfì˜ í–‰ ìˆœì„œë¥¼ ë’¤ì§‘ê¸° (ì—­ìˆœ)
count_df_reversed = count_df.iloc[::-1]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. ì¶œêµ¬ ì˜¨ë„')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

# 1. O2ì™€ NOx ë²”ìœ„ ì„¤ì •
o2_bins  = list(range(0,  22,  2))  # ì˜ˆ: [1100, 1150, 1200]
nox_bins = list(range(0, 130, 10))  # ì˜ˆ: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['br1_eo_o2_a'], bins=o2_bins, edgecolor='black', color='skyblue')
plt.xticks(o2_bins)
plt.xlabel('O2 (%)')
plt.ylabel('Count')
plt.title('ë³´ì¼ëŸ¬ O2 ë†ë„ Histogram')
plt.tight_layout()
plt.show()

# 2. ê° ë³€ìˆ˜ binìœ¼ë¡œ ë‚˜ëˆ„ê¸°
df[ 'o2_bin'] = pd.cut(df['br1_eo_o2_a'], bins= o2_bins, right=False)
df['nox_bin'] = pd.cut(df[  'nox_value'], bins=nox_bins, right=False)

# 3. 2D ê·¸ë£¹ë³„ ì¹´ìš´íŒ…
count_df = df.groupby(['nox_bin', 'o2_bin']).size().unstack(fill_value=0)
# 1. count_dfì˜ í–‰ ìˆœì„œë¥¼ ë’¤ì§‘ê¸° (ì—­ìˆœ)
count_df_reversed = count_df.iloc[::-1]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. ë³´ì¼ëŸ¬ O2 ë†ë„')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

idx_modeling = (
    df[['icf_ccs_fg_t_1', 'icf_scs_fg_t_1']]
    .loc[df['icf_ccs_fg_t_1'] >= 850, ]
    .loc[df['icf_scs_fg_t_1'] >= 750, ]
    .index
)

df.shape[0] - len(idx_modeling)

"""## column ì„¤ì • + ì˜¨ë„ ì¡°ê±´ìœ¼ë¡œ row filtering"""

## ë¡œê·¸ ì¶”ê°€

## targetì„ ì •í™•íˆ 2.5ë¶„ ë’¤ë¡œ ë§¤í•‘ (index ë§ê³  ì‹œê°„ìœ¼ë¡œ)
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time

# íŒŒë¼ë¯¸í„°
nox_col = "nox_value"
time_col = "_time_gateway"

# feature_cols = ["is_spike", "near_spike"] + cols_x_original + cols_x_stat
feature_cols = ["is_spike"] + cols_x_original + cols_x_stat

df_model = df.loc[idx_modeling, [time_col, nox_col, 'target', 'weights'] + feature_cols].sort_values(by=time_col).reset_index(drop=True)
print(f"   âœ… ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (í–‰: {len(df_model):,}, ì—´: {len(df_model.columns)})")

"""## ê²°ì¸¡ì¹˜ ì²˜ë¦¬
- 1. column ì œì™¸: columnë³„ ê²°ì¸¡ì¹˜ í™•ì¸ í›„, targetì´ ìˆëŠ” row ì¤‘ ê²°ì¸¡ì¹˜ê°€ 10,000ê°œ ì´ìƒì´ë©´ ì œì™¸
- 2. row ì œì™¸: 1. ì§„í–‰ í›„ ê²°ì¸¡ row ì œì™¸
"""

before_count = len(df_model)

# ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
na_count = df_model.dropna(subset=[col_nox_tms_af, 'target', 'weights'])[feature_cols].isna().sum()

# ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
na_count_sorted = na_count.sort_values(ascending=False)

na_count_sorted.iloc[:50]

# ì˜ˆì‹œ: na_count_sortedê°€ Seriesì¼ ë•Œ
bins = np.arange(0, na_count_sorted.max() + 10000, 10000)  # 10000 ë‹¨ìœ„ êµ¬ê°„ ìƒì„±
labels = [f"{int(bins[i])}~{int(bins[i+1])}" for i in range(len(bins)-1)]

# êµ¬ê°„ë³„ ê°œìˆ˜ ì§‘ê³„
na_count_bins = pd.cut(na_count_sorted, bins=bins, labels=labels, right=False)
na_bin_counts = na_count_bins.value_counts().sort_index()

na_bin_counts.loc[na_bin_counts > 0]

cols_na = [x for x in na_count_sorted.index[na_count_sorted > 10000].tolist() if x != 'target']

len(cols_na)

feature_cols = [x for x in feature_cols if x not in cols_na]
valid_idx = df_model[feature_cols + [col_nox_tms_af, "target", "weights"]].notna().all(axis=1)

valid_idx.value_counts(dropna=False)

df_model = df_model.loc[valid_idx].reset_index(drop=True)
after_count = len(df_model)
removed_count = before_count - after_count

print(f"   ì „ì²´ í–‰: {before_count:,} â†’ {after_count:,}")
print(f"   ì œê±°ëœ í–‰: {removed_count:,} ({removed_count/before_count*100:.1f}%)")

"""# LightGBM ëª¨ë¸ë§

## lightgbm library & GPU ì„¤ì •

### (í•„ìš” ì‹œ ì‚¬ìš©) GPU ë²„ì „ lightgbm ì„¤ì¹˜
"""

# ### LGBM GPU ë²„ì „ ###

# # 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# !apt-get install -y -qq libboost-all-dev

# # 2. Gitì—ì„œ LightGBM clone
# !git clone --recursive https://github.com/microsoft/LightGBM

# # 3. GPU í™œì„±í™”í•œ ìƒíƒœë¡œ ë¹Œë“œ
# %cd LightGBM
# !mkdir build
# !cmake -DUSE_GPU=1 -DOpenCL_INCLUDE_DIR=/usr/include -DOpenCL_LIBRARY=/usr/lib/x86_64-linux-gnu/libOpenCL.so .
# !make -j4

# # 4. íŒŒì´ì¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# %cd python-package
# !python setup.py install

# # 5. Colab ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ì˜¤ê¸°
# %cd /content

"""### import"""

!pip install LightGBM

# - lightgbm GPUë²„ì „ íŒ¨í‚¤ì§€ ì„¤ì¹˜ëœ ê²½ë¡œ
# os.chdir('/content/drive/MyDrive/Colab Notebooks/baseline_modeling_250512_250704')

import lightgbm as lgb
print("LightGBM version:", lgb.__version__)
import subprocess

# === GPU ê°ì§€ í•¨ìˆ˜ ===
def is_gpu_available():
    try:
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return result.returncode == 0
    except FileNotFoundError:
        return False

use_gpu = is_gpu_available()
print("GPU ì‚¬ìš© ì—¬ë¶€:", use_gpu)

"""## train, test ë¶„í• """

# # Train/Test ë¶„í• 
# print("\nâœ‚ï¸ ë°ì´í„° ë¶„í•  ì¤‘...")

# # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ df_modelì—ì„œ 80%ì— í•´ë‹¹í•˜ëŠ” ì‹œì ì„ ì°¾ê¸°
# split_index = int(len(df_model) * 0.8)
# datetime_train_end = df_model.iloc[split_index][col_datetime]

# train_df = df_model.loc[df_model[col_datetime] <= datetime_train_end].copy()
# test_df  = df_model.loc[df_model[col_datetime] >  datetime_train_end].copy()

# print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ ({len(train_df)/len(df_model)*100:.1f}%)")
# print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ ({len(test_df)/len(df_model)*100:.1f}%)")

# print(f"\n   í›ˆë ¨ ë°ì´í„° ì‹œì‘: {train_df[col_datetime].min()}")
# print(f"   í›ˆë ¨ ë°ì´í„° ì¢…ë£Œ: {train_df[col_datetime].max()}")
# print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‹œì‘: {test_df[col_datetime].min()}")
# print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¢…ë£Œ: {test_df[col_datetime].max()}")

# Train/Test ë¶„í•  (ì‚¬ìš©ì ì§€ì • ê¸°ê°„)
print("\nâœ‚ï¸ ë°ì´í„° ë¶„í•  ì¤‘ (ì‚¬ìš©ì ì§€ì • ê¸°ê°„)...")

# ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
df_model.sort_values(by=col_datetime, inplace=True)

# ì§€ì •ëœ í•™ìŠµ ì¢…ë£Œ ì‹œì 
datetime_train_end = pd.to_datetime('2025-07-04 23:59:59')

# ì§€ì •ëœ í…ŒìŠ¤íŠ¸ ì‹œì‘/ì¢…ë£Œ ì‹œì 
datetime_test_start = pd.to_datetime('2025-07-05')
datetime_test_end = pd.to_datetime('2025-07-16')

# ë°ì´í„° ë¶„í• 
train_df = df_model.loc[df_model[col_datetime] <= datetime_train_end].copy()
test_df  = df_model.loc[
    (df_model[col_datetime] >= datetime_test_start) &
    (df_model[col_datetime] <= datetime_test_end + pd.Timedelta(days=1, seconds=-1)) # Include the end of the last day
].copy()


print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ")

print(f"\n   í›ˆë ¨ ë°ì´í„° ì‹œì‘: {train_df[col_datetime].min()}")
print(f"   í›ˆë ¨ ë°ì´í„° ì¢…ë£Œ: {train_df[col_datetime].max()}")
print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‹œì‘: {test_df[col_datetime].min()}")
print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¢…ë£Œ: {test_df[col_datetime].max()}")

"""## oversampling (training set)"""

# Oversampling
print("\nğŸ”„ ì˜¤ë²„ìƒ˜í”Œë§ ìˆ˜í–‰ ì¤‘...")
over_df = train_df[train_df["target"] >= 40]
over_sampled = over_df.sample(frac=1.0, replace=True, random_state=42)
original_train_size = len(train_df)
train_df = pd.concat([train_df, over_sampled])

print(f"   ê³ ë†ë„ ìƒ˜í”Œ(>=40ppm): {len(over_df):,}ê°œ")
print(f"   ì˜¤ë²„ìƒ˜í”Œë§ í›„ í›ˆë ¨ ë°ì´í„°: {original_train_size:,} â†’ {len(train_df):,}")











"""# ì‹¤í—˜ 3-0.

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

# model = lgb.LGBMRegressor(
#     random_state=42,
#     # verbose=-1,  # í•™ìŠµ ë¡œê·¸ ìˆ¨ê¹€
#     n_estimators=1000,
#     learning_rate=0.03,
#     num_leaves=127,
#     max_depth=12,
#     min_child_samples=30,
#     reg_alpha=0.1,    # L1
#     reg_lambda=1.0,   # L2
#     device='gpu',
#     n_jobs=-1
# )


start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

import pickle

# ëª¨ë¸ ì €ì¥
with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
    pickle.dump(model, f)

import lightgbm as lgb
print(f"LightGBM version in current environment: {lgb.__version__}")

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""



# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")







"""# ì‹¤í—˜ 4. 2025ë…„ 4ì›” í•™ìŠµ ë°ì´í„° ì¶”ê°€

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

# Prepare test data for inference and save to CSV
print("\nğŸ’¾ ì¶”ë¡ ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ë° ì €ì¥ ì¤‘...")

# Select relevant columns for inference: time, feature columns, and actual nox_value
cols_for_inference = [col_datetime, col_nox_tms_af] + feature_cols

# Ensure data is clean (should already be from previous steps, but double-check)
test_df_inference = test_df[cols_for_inference].dropna().copy()

# Define save path
inference_data_path = os.path.join(dir_save, "test_data_for_inference.csv")

# Save to CSV
test_df_inference.to_csv(inference_data_path, index=False)

print(f"   âœ… ì¶”ë¡ ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {inference_data_path}")
print(f"   ì €ì¥ëœ ë°ì´í„° í–‰: {len(test_df_inference):,}, ì—´: {len(test_df_inference.columns)}")

print(test_df_inference.columns)

test_df_inference.head(3)

# Prepare test data for inference and save to Parquet
print("\nğŸ’¾ ì¶”ë¡ ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ë° Parquet íŒŒì¼ ì €ì¥ ì¤‘...")

# Select relevant columns for inference: time, feature columns, and actual nox_value
# Assuming 'test_df_inference' is already prepared in the preceding cell
# cols_for_inference = [col_datetime, col_nox_tms_af] + feature_cols
# test_df_inference = test_df[cols_for_inference].dropna().copy()

# Define save path for Parquet
inference_data_path_parquet = os.path.join(dir_save, "test_data_for_inference.parquet")

# Save to Parquet
test_df_inference.to_parquet(inference_data_path_parquet, index=False)

print(f"   âœ… ì¶”ë¡ ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {inference_data_path_parquet}")
print(f"   ì €ì¥ëœ ë°ì´í„° í–‰: {len(test_df_inference):,}, ì—´: {len(test_df_inference.columns)}")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

# import pickle

# # ëª¨ë¸ ì €ì¥
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")







"""# ì‹¤í—˜ 5ë²ˆ. 2025ë…„ 3ì›”ë¶€í„° í•™ìŠµ ë°ì´í„° ì¶”ê°€

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

feature_cols

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

# import pickle

# # ëª¨ë¸ ì €ì¥
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")







"""# ì‹¤í—˜ 6ë²ˆ. 2025ë…„ 5ì›” 22ì¼ ë¶€í„° í•™ìŠµ

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

# import pickle

# # ëª¨ë¸ ì €ì¥
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")





"""# ì‹¤í—˜ 7ë²ˆ. Baseline ëª¨ë¸ì— ì‹ ê·œ ë°ì´í„° ì¶”ê°€

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

# import pickle

# # ëª¨ë¸ ì €ì¥
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")





"""# ì‹¤í—˜ 8ë²ˆ. 5/22 ë¶€í„°ì˜ ëª¨ë¸ì— ì‹ ê·œ ë°ì´í„° ì¶”ê°€

## ëª¨í˜• í•™ìŠµ
"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

# import pickle

# # ëª¨ë¸ ì €ì¥
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV ì €ì¥
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### ê·¸ë˜í”„ ë° ê²°ê³¼ ì €ì¥"""

# ì„±ëŠ¥ ì§€í‘œë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# êµ¬ê°„ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ì— MAE, RMSE, sMAPE ì—´ ì¶”ê°€
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# ê·¸ë˜í”„ ì €ì¥ (ì´ì „ì— ìƒì„±ëœ matplotlib figure ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì €ì¥)

print("\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥ ì¤‘...")

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬ and ì‚°ì ë„
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {dir_save} í´ë”")

















"""# ìµœì¢… -> ì‹¤í—˜ 3-1-2. ì ìš© + sample weight ë³€ê²½"""

dir_save = os.path.join(
    '/content/drive/MyDrive/Colab Notebooks/250801_baseline_modeling_ì¸ìˆ˜ì¸ê³„',
    'ì‹¤í—˜3_1_2_1'
)
os.makedirs(dir_save, exist_ok=True)

dir_save

"""## ëª¨í˜• í•™ìŠµ"""

# ëª¨ë¸ í•™ìŠµ
print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
# model = lgb.LGBMRegressor(
#     random_state=42,
#     n_estimators=100,
#     device='gpu' if use_gpu else 'cpu'
# )

model = lgb.LGBMRegressor(
    random_state=42,
    # verbose=-1,  # í•™ìŠµ ë¡œê·¸ ìˆ¨ê¹€
    n_estimators=500,
    learning_rate=0.03,
    num_leaves=63,
    max_depth=10,
    min_child_samples=20,
    reg_alpha=0.1,    # L1
    reg_lambda=1.0,   # L2
    device='gpu',
    n_jobs=-1
)


start_time = time.time()
# model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"].apply(lambda x: 2*x if x>=2 else x))
training_time = time.time() - start_time

print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")

# ì˜ˆì¸¡
print("\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

"""## ê²°ê³¼ ì €ì¥ - ëª¨í˜•, ë³€ìˆ˜ ì¤‘ìš”ë„"""

import pickle

# ëª¨ë¸ ì €ì¥
with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
    pickle.dump(model, f)

# ë³€ìˆ˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
importance_df = pd.DataFrame({
    "feature": feature_cols,
    "importance": model.feature_importances_
}).sort_values("importance", ascending=False)

# CSV ì €ì¥
importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# pickleë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´
with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
    pickle.dump(importance_df, f)

"""## test set ì„±ëŠ¥ í‰ê°€"""

# í‰ê°€
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("ğŸ‰ ìµœì¢… ê²°ê³¼")
print("=" * 60)
print(f"âœ… MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.3f} ppm")
print(f"âœ… RMSE (ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨): {rmse:.3f} ppm")
print()

# í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
print("ğŸ† í”¼ì²˜ ì¤‘ìš”ë„ Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des í…Œì´ë¸”ì—ì„œ ê³„ì¸¡ í•­ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['ê³„ì¸¡ í•­ëª© ']))

# í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (ì„¤ëª… í¬í•¨)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# yì¶• ë¼ë²¨ ìƒì„± (ë³€ìˆ˜ëª… + ì„¤ëª…)
y_labels = []
for feature in top_features['feature']:
   # lag ì •ë³´ ì¶”ì¶œ
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (í”¼ì²˜ ì¤‘ìš”ë„)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# ê·¸ë˜í”„ ê°„ê²© ì¡°ì •
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# ê²°ê³¼ ì •ë¦¬
result_df = test_df.copy()
result_df["NOx_ì‹¤ì œê°’"] = test_df["target"]
result_df["NOx_ì˜ˆì¸¡ê°’"] = y_pred

print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

# ì‹œê°í™” 1: ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
plt.figure(figsize=(15, 6))
interval = 30  # ì˜ˆ: 100ê°œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = result_df.iloc[::interval].copy()  # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
sample_result = sample_result.sort_values(time_col)  # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ì„ íƒì )

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_ì˜ˆì¸¡ê°’"],
         label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2ë¶„ 30ì´ˆ ë’¤ NOx ì˜ˆì¸¡ ê²°ê³¼ (ìƒ˜í”Œ)")
plt.xlabel("ì‹œê°„")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# ì‹œê°í™” 2: ì”ì°¨ ë¶„í¬
residual = result_df["NOx_ì‹¤ì œê°’"] - result_df["NOx_ì˜ˆì¸¡ê°’"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨) ë¶„í¬")
plt.xlabel("ì˜¤ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)")
plt.grid(True, alpha=0.3)

# ì‚°ì ë„
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_ì‹¤ì œê°’"], result_df["NOx_ì˜ˆì¸¡ê°’"], alpha=0.5)
plt.plot([result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         [result_df["NOx_ì‹¤ì œê°’"].min(), result_df["NOx_ì‹¤ì œê°’"].max()],
         'r--', linewidth=2)
plt.xlabel("ì‹¤ì œ NOx")
plt.ylabel("ì˜ˆì¸¡ NOx")
plt.title("ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì‹œê°í™” 3: êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
print("ğŸ“Š êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_ì‹¤ì œê°’"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_ì‹¤ì œê°’", "count"),
    ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
    MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
    RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
    sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_count=("NOx_ì‹¤ì œê°’", lambda x: np.sum((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
    pos_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) > 0)),
    neg_residual_ratio=("NOx_ì‹¤ì œê°’", lambda x: np.mean((x - result_df.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) < 0)),
).reset_index()

print("\nğŸ“‹ êµ¬ê°„ë³„ ì„±ëŠ¥:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ë°ì´í„° ê°œìˆ˜")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ê°œìˆ˜")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx êµ¬ê°„ë³„ ì˜ˆì¸¡ ì„±ëŠ¥")
plt.xlabel("NOx êµ¬ê°„")
plt.ylabel("ì˜¤ì°¨")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸŠ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""## test set 6ì‹œê°„ ê°„ê²© ê·¸ë¦¼"""

delta_sec = 150

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import OrderedDict

# ê¸°ë³¸ í°íŠ¸ í¬ê¸° ì¡°ì ˆ
plt.rcParams.update({
    'font.size': 10,
    'axes.labelsize': 11,
    'xtick.labelsize': 12,
    'ytick.labelsize': 13,
    'legend.fontsize': 11,
    'axes.titlesize': 13
})

# 6ì‹œê°„ êµ¬ê°„ ìƒì„± í•¨ìˆ˜
def get_6hr_interval_bounds(date):
    start = pd.Timestamp(date)
    return [(start + pd.Timedelta(hours=6 * i), start + pd.Timedelta(hours=6 * (i + 1))) for i in range(4)]

def plot_with_gap(ax, df, time_col, value_col, label=None, **plot_kwargs):
    df_sorted = df.sort_values(time_col).reset_index(drop=True)
    df_sorted["time_diff"] = df_sorted[time_col].diff().dt.total_seconds()
    segment_id = (df_sorted["time_diff"] > 5).cumsum()

    for _, segment in df_sorted.groupby(segment_id):
        ax.plot(segment[time_col], segment[value_col], label=label, **plot_kwargs)
        label = None  # ë™ì¼ ë¼ë²¨ ë°˜ë³µ ë°©ì§€

# ë‚ ì§œ ì»¬ëŸ¼ ìƒì„±
result_df["date"] = result_df[time_col].dt.date
unique_dates = sorted(result_df["date"].unique())
result_df["time_target"] = result_df[time_col] + pd.Timedelta(seconds=delta_sec)

for date in unique_dates:
    intervals = get_6hr_interval_bounds(date)
    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(22, 11),
                            gridspec_kw={'width_ratios': [3.3, 1]},
                            constrained_layout=True)
    fig.subplots_adjust(hspace=0.2)

    for i, (start_time, end_time) in enumerate(intervals):
        mask = (result_df[time_col] >= start_time) & (result_df[time_col] < end_time)
        df_interval = result_df[mask].copy()

        ax_ts = axs[i][0]
        ax_table = axs[i][1]

        # ì‹œê³„ì—´ í”Œë¡¯
        # ax_ts.plot(df_interval[time_col], df_interval["NOx_ì‹¤ì œê°’"], label="ì‹¤ì œ NOx", alpha=0.7, linewidth=1.5)
        # ax_ts.plot(df_interval[time_col], df_interval["NOx_ì˜ˆì¸¡ê°’"], label="ì˜ˆì¸¡ NOx", alpha=0.7, linewidth=1.5)
        # ì‹œê³„ì—´ í”Œë¡¯
        # plot_with_gap(ax_ts, df_interval, time_col, "NOx_ì‹¤ì œê°’", label="ì‹¤ì œ NOx", color='tab:blue',   alpha=0.7, linewidth=1.5)
        # âœ… ì‹¤ì œê°’ì€ time_target ê¸°ì¤€ìœ¼ë¡œ í”Œë¡¯
        plot_with_gap(ax_ts, df_interval, time_col, "nox_value",  label="ì‹¤ì œ NOx", color='tab:blue',   alpha=0.7, linewidth=1.5)
        plot_with_gap(ax_ts, df_interval, time_col, "NOx_ì˜ˆì¸¡ê°’", label="ì˜ˆì¸¡ NOx", color='tab:orange', alpha=0.7, linewidth=1.5)
        ax_ts.set_xlim([start_time, end_time])

        # âœ… yì¶•: ìµœì†Œ 42.5ê¹Œì§€ ë³´ì¥
        y_min, y_max = df_interval[["nox_value", "NOx_ì˜ˆì¸¡ê°’"]].min().min(), df_interval[["nox_value", "NOx_ì˜ˆì¸¡ê°’"]].max().max()
        ax_ts.set_ylim(bottom=0, top=max(42.5, y_max + 2))

        # âœ… ê²½ê³„ì„  ì¶”ê°€
        ax_ts.axhline(30, color="red", linestyle="--", linewidth=1)
        ax_ts.axhline(40, color="red", linestyle="-", linewidth=1)

        ax_ts.set_title(f"{start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%H:%M')}")
        ax_ts.set_ylabel("NOx (ppm)")
        ax_ts.grid(True, alpha=0.2)
        if i == 0:
            ax_ts.legend(loc='upper right')
        if i == 3:
            ax_ts.set_xlabel("ì‹œê°„")

        # xì¶• 30ë¶„ ê°„ê²© ì‹œê°„ë§Œ í‘œì‹œ (ë‚ ì§œ ì—†ìŒ)
        ax_ts.xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0,30]))
        ax_ts.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        plt.setp(ax_ts.xaxis.get_majorticklabels(), rotation=0, ha='right')
        # âœ… ì²« ë²ˆì§¸ tick label ìˆ¨ê¹€
        xticklabels = ax_ts.get_xticklabels()
        if xticklabels:
            xticklabels[0].set_visible(False)

        # yì¶• tick label 0ê³¼ xì¶• tick ê²¹ì¹¨ ë°©ì§€ (yì¶• ë ˆì´ë¸” ìœ„ë¡œ ì•½ê°„ ì´ë™)
        ax_ts.yaxis.set_tick_params(pad=10)

        # xì¶• ì²« tick ì•½ê°„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™(ê°„ê²©ì´ 5ì´ˆ ì´ìƒì´ë©´ ì„  ëŠê¹€ ë°˜ì˜)
        ticks = ax_ts.get_xticks()
        if len(ticks) > 1:
            ticks = np.array(ticks)
            ticks[0] += (ticks[1] - ticks[0]) * 0.05
            ax_ts.set_xticks(ticks)

        # í…Œì´ë¸” ìƒì„±
        ax_table.axis("off")
        if not df_interval.empty:
            bins = np.arange(0, 120, 10)
            df_interval["NOx_bin"] = pd.cut(df_interval["NOx_ì‹¤ì œê°’"], bins=bins, right=False)

            grouped = df_interval.groupby("NOx_bin").agg(
                count=("NOx_ì‹¤ì œê°’", "count"),
                ME=("NOx_ì‹¤ì œê°’", lambda x: np.mean(x - df_interval.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])),
                MAE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(np.abs(x - df_interval.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]))),
                RMSE=("NOx_ì‹¤ì œê°’", lambda x: np.sqrt(np.mean((x - df_interval.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"])**2))),
                sMAPE=("NOx_ì‹¤ì œê°’", lambda x: np.mean(
                    2 * np.abs(x - df_interval.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]) /
                    (np.abs(x) + np.abs(df_interval.loc[x.index, "NOx_ì˜ˆì¸¡ê°’"]).replace(0, np.nan))
                ) * 100),
            ).reset_index()

            # ë¬¸ìì—´ bin ë²”ìœ„
            grouped["NOx_bin"] = grouped["NOx_bin"].astype(str)
            numeric_cols = ["count", "ME", "MAE", "RMSE", "sMAPE(%)"]
            grouped.rename(columns={"sMAPE": "sMAPE(%)"}, inplace=True)
            rounded = grouped[numeric_cols].round(2)

            # ê²°ì¸¡ì¹˜ ë˜ëŠ” count==0ì¸ ì…€ì„ "-"ë¡œ í‘œì‹œ
            display_data = pd.concat([grouped[["NOx_bin"]], rounded], axis=1).astype(object)
            for col in numeric_cols:
                display_data.loc[display_data["count"] == 0, col] = "-"
                display_data[col] = display_data[col].replace([np.nan, np.inf, -np.inf], "-")

            # í…Œì´ë¸” ê·¸ë¦¬ê¸°
            table = ax_table.table(
                cellText=display_data.values.tolist(),
                colLabels=display_data.columns,
                cellLoc="center",
                loc="center"
            )
            table.auto_set_font_size(False)
            table.set_fontsize(10.5)
            table.scale(1, 1.05)

    # ì €ì¥
    save_path = os.path.join(dir_save, f"NOx_prediction_6hr_{date}.png")
    plt.tight_layout()
    # ì €ì¥ (tight ì˜µì…˜ ìœ ì§€)
    plt.savefig(save_path, dpi=300)  # bbox_inches='tight'ëŠ” constrained_layoutì´ë©´ ë¶ˆí•„ìš”
    plt.close()

dir_save





df.head(3)

"""## 4ì›” í•œë‹¬ê°„ì˜ ìš”ì†Œìˆ˜ ì£¼ì…ëŸ‰"""

import matplotlib.dates as mdates

# Filter data for the specified date range
start_date = pd.to_datetime('2025-04-06')
end_date = pd.to_datetime('2025-04-30')

df_filtered = df[
    (df[col_datetime] >= start_date) &
    (df[col_datetime] <= end_date + pd.Timedelta(days=1, seconds=-1)) # Include the end of the last day
].copy()

# Sample every 10th data point
df_sampled = df_filtered.iloc[::10].copy()

# Get unique dates in the filtered data
unique_dates = df_sampled[col_datetime].dt.date.unique()

# Create subplots for each date
n_dates = len(unique_dates)
fig, axes = plt.subplots(nrows=n_dates, figsize=(15, 4 * n_dates), sharex=False)

# Ensure axes is an array even if there's only one subplot
if n_dates == 1:
    axes = [axes]

for i, date in enumerate(unique_dates):
    ax = axes[i]
    df_date = df_sampled[df_sampled[col_datetime].dt.date == date]

    ax.plot(df_date[col_datetime], df_date[col_pump_hz])
    ax.set_title(f"Urea Pump Hz - {date}")
    ax.set_ylabel("Hz")
    ax.grid(True, alpha=0.3)

    # Format x-axis to show time
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
    fig.autofmt_xdate() # Auto-rotate date labels

plt.tight_layout()
plt.show()

"""# ìµœì¢… ë°ì´í„° ì €ì¥

## feature ì™¸ ê¸°ë³¸ ì •ë³´ column ì„¤ì •
"""

y_pred_all = pd.Series(model.predict(df_model[feature_cols]), index=df_model.index)

df_model['nox_pred_future'] = y_pred_all

df_model['train_test_etc'] = df_model[col_datetime].apply(lambda x: 'train' if x <= datetime_train_end else 'test')
df_model['is_row_for_modeling'] = True

cols_basic = [

    # ì‹œê°
    col_datetime

    # - ëª¨ë¸ë§ ëŒ€ìƒ ì—¬ë¶€
    ,'is_row_for_modeling'
    # - í•™ìŠµ ëŒ€ìƒ ì—¬ë¶€
    ,'train_test_etc'

    # í˜„ì¬(t)ì‹œì  NOx ì‹¤ì œê°’
    ,'nox_value'
    # í˜„ì¬(t)ì‹œì ê¹Œì§€ featureë¥¼ ì‚¬ìš©í•œ, 2.5ë¶„ ë’¤ NOx ì˜ˆì¸¡ê°’
    ,'nox_pred_future'
    # 2.5ë¶„ ë’¤ ë¯¸ë˜ ì‹œì ì˜ NOx
    # - ì‹¤ì œê°’
    ,'nox_value_future'

    ### featureë¡œ ì‚¬ìš©í•˜ì§„ ì•Šì•˜ì§€ë§Œ, spike ë° weight ì •ì˜ ì‹œ ì‚¬ìš©ëœ ë¶€ê°€ ì •ë³´ ###
    ,'nox_range_1min'
    ,'nox_std_1min'
    ,'weights'
]

"""## ëª¨ë¸ë§ í¬í•¨ ë°ì´í„° + ì œì™¸ ë°ì´í„°"""

df_final = pd.merge(
    df.rename(columns={'target': 'nox_value_future'}),
    df_model[[col_datetime, 'nox_pred_future', 'train_test_etc', 'is_row_for_modeling']],
    how='left',
    on=col_datetime
)

df_final = df_final[[x for x in cols_basic] + [x for x in df_final.columns if x not in cols_basic]]

df_final.sort_values(by=col_datetime, inplace=True)
df_final.reset_index(drop=True, inplace=True)

df_final['is_row_for_modeling'] = df_final['is_row_for_modeling'].fillna(False)
df_final[     'train_test_etc'] = df_final[     'train_test_etc'].fillna('etc')

"""## NOx 1ë¶„ ì „ diff"""

# Datetime â†’ index
df_final = df_final.sort_values(col_datetime).set_index(col_datetime)

# 1ë¶„ ì „ì˜ ì‹œê° ìƒì„±
target_time = df_final.index - pd.Timedelta(minutes=1)

# dict ë§¤í•‘
nox_bf_map = df_final[col_nox_tms_bf].to_dict()
nox_af_map = df_final[col_nox_tms_af].to_dict()

# 1ë¶„ ì „ ê°’ ì°¾ê¸°
nox_bf_1min_ago = target_time.map(nox_bf_map)
nox_af_1min_ago = target_time.map(nox_af_map)

# ìƒˆë¡œìš´ ì»¬ëŸ¼ 2ê°œ ìƒì„±
df_final.insert(7, 'nox_diff_bf_1min', df_final[col_nox_tms_bf] - nox_bf_1min_ago.values)
df_final.insert(8, 'nox_diff_af_1min', df_final[col_nox_tms_af] - nox_af_1min_ago.values)

# index ë³µì›
df_final.reset_index(drop=False, inplace=True)

"""## ì €ì¥"""

parquet_path = os.path.join(dir_save, 'df_final.parquet')
meta_path = os.path.join(dir_save, 'df_final_category_meta.json')

import json

# === 1ï¸âƒ£ category column ì •ë³´ ìˆ˜ì§‘ ===
cat_info = {}
for col in df_final.columns:
    if df_final[col].dtype.name == 'category':
        cat_info[col] = list(df_final[col].cat.categories.astype(str))

# === 2ï¸âƒ£ DataFrame ì €ì¥ (category â†’ str ë³€í™˜) ===
df_final_for_save = df_final.apply(lambda x: x.astype(str) if x.dtype.name == 'category' else x)
df_final_for_save.to_parquet(parquet_path, index=False)

# === 3ï¸âƒ£ category ë©”íƒ€ë°ì´í„° ì €ì¥ ===
with open(meta_path, 'w', encoding='utf-8') as f:
    json.dump(cat_info, f, ensure_ascii=False, indent=2)

print(f"âœ… DataFrame ì €ì¥ ì™„ë£Œ: {parquet_path}")
print(f"âœ… Category ì •ë³´ ì €ì¥ ì™„ë£Œ: {meta_path}")

df_final



