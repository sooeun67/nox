# -*- coding: utf-8 -*-
"""실험3-0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-qlf04MglI8nGJvVgzUI1cBN08gTsxy

# (필요 시 사용) Colab 환경 설정

## matplitlib 그림에 한글 표시하기 - colab에서 한글 표시 안돼서 어쩔 수 없이 사용중...
"""

# 1. 나눔글꼴 설치
!apt-get -qq -y install fonts-nanum

# 2. 런타임 캐시 지우기 및 재로드를 위한 matplotlib 캐시 삭제
import matplotlib
import shutil
import os

matplotlib_cache_dir = matplotlib.get_cachedir()
if os.path.exists(matplotlib_cache_dir):
    shutil.rmtree(matplotlib_cache_dir)

# 3. 런타임 재시작 (중요)
os.kill(os.getpid(), 9)

"""## Google Drive 마운트"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/test_data_for_inference.csv")

sample = df[:200]
sample.to_csv("sample.csv")





"""# 환경 설정"""

!pip install polars pyarrow openpyxl

import os
import ast
import numpy as np
import pandas as pd
import polars as pl
import seaborn as sns

from functools import reduce
from datetime import date, datetime, timedelta
import datetime as dt
from typing import Dict, List, Tuple

# 1. 전역 설정 변경 (노트북 전체에 적용)
pl.Config.set_tbl_rows(30)  # 기본값은 보통 10
pl.Config.set_tbl_cols(20)   # 컬럼 개수도 조정 가능

import warnings
warnings.filterwarnings("ignore")

# 재시작 후: 폰트 설정
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False

dir_save = os.path.join(
    '/content/drive/MyDrive/SKEP/0.Colab Notebooks/2.Modeling/[믿음_예측모델_0801]',
    '실험8'
)
os.makedirs(dir_save, exist_ok=True)

"""## 메타데이터"""

meta_dat = pd.read_excel('/content/drive/MyDrive/SKEP/0.Colab Notebooks/2.Modeling/[믿음_예측모델_0801]/메타데이터_v2_250804.xlsx', sheet_name='요소수_RAW_DATA')
des = (
    meta_dat[['DATAFIELD_lowercased','Description','계측 항목 ','SOURCE']]
    .dropna(subset=['DATAFIELD_lowercased', 'Description'], how='all')
    .reset_index(drop=True)
)
des

"""## 이상치/결측치 전처리 후 데이터"""

# 경로 설정
# path_parquet_data = '/content/drive/MyDrive/SKEP/250708_5sMean/cleaned_240411_250704.parquet'
path_parquet_data = '/content/drive/MyDrive/SKEP/250725_SRS1_신규수집/cleaned_240411_250724.parquet'

# 선택 컬럼
col_datetime   = "_time_gateway"
col_nox_tms_af = 'nox_value'
col_nox_tms_bf = 'icf_tms_nox_a'
col_pump_hz = 'snr_pmp_uw_s_1'
cols_x_raw = [

     'bft_eo_fg_t'
    ,'br1_eo_fg_t'
    ,'br1_eo_o2_a'
    ,'br1_eo_st_t'
    ,'dr1_eq_bw_c'
    ,'icf_ccs_fg_t_1'
    ,'icf_cra_wt_k'
    ,'icf_ff1_ar_f_1'
    ,'icf_ff1_ss_s_1'
    ,'icf_ff1_ss_s_2'
    ,'icf_ff2_ss_s_1'
    ,'icf_idf_ss_s_1'
    ,'icf_scs_fg_t_1'
    ,'sdr_htr_fg_t'
]

# ✅ datetime 조건 필터 적용 (날짜를 명시적으로 datetime으로)
df = (
    pl.scan_parquet(path_parquet_data)
    .filter(pl.col("_time_gateway") >= pl.datetime(2025, 5, 22))
    # .filter(pl.col("_time_gateway") >= pl.datetime(2025, 4, 1))
    .select([col_datetime, col_nox_tms_af, col_nox_tms_bf, col_pump_hz] + cols_x_raw)
    .collect()
    .to_pandas()
)

df.shape

"""# 파생변수 생성

## 폐기물 투입
"""

# 폐기물 투입 관련 Feature 생성
col_trash_drop = 'trash_drop'
col_trash_drop_count_30min = 'trash_drop_count_30min'

### ✅ 폐기물 투입 여부 및 누적 횟수 ###

# df = df_filtered.copy()
df = df.set_index('_time_gateway')

window_size_sec = 10
diff_tolerance = -10
df[col_trash_drop] = (
    df['icf_cra_wt_k'].bfill().rolling(window_size_sec).max().diff() < diff_tolerance
).astype(int)

df[col_trash_drop_count_30min] = (
    df[col_trash_drop].rolling("30min").sum().fillna(0)
)

df = df.reset_index()
# df.drop(columns=col_trash_waiting_time, inplace=True)

"""## 요약통계량"""

def generate_interval_summary_features_inplace_time(
    df,
    datetime_col,
    columns,
    interval_seconds=[int(60*x) for x in [1, 3, 5, 10, 30]]
):
    df.sort_values(datetime_col, inplace=True)

    new_columns = []

    for col in columns:
        print(col)
        for sec in interval_seconds:
            window = pd.Timedelta(seconds=sec)

            # 평균/표준편차
            mean_col = f"{col}_mean_{sec}s"
            std_col = f"{col}_std_{sec}s"
            df[mean_col] = df.rolling(window=window, on=datetime_col)[col].mean()
            df[std_col] = df.rolling(window=window, on=datetime_col)[col].std()
            new_columns.extend([mean_col, std_col])

            # 시작값
            df_prev = df[[datetime_col, col]]
            df_prev_shifted = df_prev.copy()
            df_prev_shifted[datetime_col] += pd.Timedelta(seconds=sec)
            df_prev_shifted.rename(columns={col: "_start_tmp"}, inplace=True)

            df_matched = pd.merge_asof(
                df, df_prev_shifted,
                on=datetime_col,
                direction="backward",
                tolerance=pd.Timedelta(seconds=0)
            )

            start_val = df_matched["_start_tmp"].values
            end_val = df[col].values

            # 변화율/변화량
            mean_rate_col = f"{col}_mean_rate_change_{sec}s"
            range_change_col = f"{col}_range_change_{sec}s"
            df[mean_rate_col] = (end_val - start_val) / start_val
            df[range_change_col] = end_val - start_val
            new_columns.extend([mean_rate_col, range_change_col])

            # ✅ 초당 변화율 기반 momentum
            df["_val_diff"] = df[col].diff()
            df["_time_diff"] = df[datetime_col].diff().dt.total_seconds()
            df["_rate_per_sec"] = df["_val_diff"] / df["_time_diff"]

            momentum_up_col = f"{col}_momentum_max_up_{sec}s"
            momentum_down_col = f"{col}_momentum_max_down_{sec}s"
            df[momentum_up_col] = df.rolling(window=window, on=datetime_col)["_rate_per_sec"].max()
            df[momentum_down_col] = df.rolling(window=window, on=datetime_col)["_rate_per_sec"].min()
            new_columns.extend([momentum_up_col, momentum_down_col])

            # 시작값 대비 최대 증가/감소량
            rolling_window = df.rolling(window=window, on=datetime_col)[col]
            max_inc_col = f"{col}_max_increase_from_start_{sec}s"
            max_dec_col = f"{col}_max_decrease_from_start_{sec}s"
            df[max_inc_col] = rolling_window.max() - start_val
            df[max_dec_col] = rolling_window.min() - start_val
            new_columns.extend([max_inc_col, max_dec_col])

    # 임시 컬럼 제거
    df.drop(columns=["_val_diff", "_time_diff", "_rate_per_sec"], inplace=True, errors="ignore")

    return new_columns

cols_x_original = [

     'bft_eo_fg_t'
    ,'br1_eo_fg_t'
    ,'br1_eo_o2_a'
    ,'br1_eo_st_t'
    ,'dr1_eq_bw_c'
    ,'icf_ccs_fg_t_1'
    ,'icf_cra_wt_k'
    ,'icf_ff1_ar_f_1'
    ,'icf_ff1_ss_s_1'
    ,'icf_ff1_ss_s_2'
    ,'icf_ff2_ss_s_1'
    ,'icf_idf_ss_s_1'
    ,'icf_scs_fg_t_1'
    ,'icf_tms_nox_a'
    ,'sdr_htr_fg_t'

    ,'trash_drop'
    ,'trash_drop_count_30min'
]

len(cols_x_original)

"""### (Feature 사용 X) 요소수 Pump Hz
- 예측 모형에서는 사용하지 않지만, 추후 제어 EDA 및 모델링 염두에 두고 생성
"""

cols_hz_stat = generate_interval_summary_features_inplace_time(df, col_datetime, [col_pump_hz])

"""### 요약통계량 Feature
- 원본 15개 + 폐기물 투입 파생변수 2개
"""

# inplace로 생성 후 column 목록 반환
cols_x_stat = generate_interval_summary_features_inplace_time(df, col_datetime, cols_x_original)

len(cols_x_stat)

df[cols_x_stat + cols_hz_stat].head()

"""## is_spike
- (참고) near_spike: 분석 제외
"""

def mark_nox_spikes(
    df,
    time_col="_time_gateway",
    nox_col="nox_value",
    window_time_sec=60,         # 🔹 1분 Rolling window (초 단위)
    spike_range_threshold=8,
    spike_std_threshold=6,
    spike_window_sec=120        # 🔹 스파이크 주변 ±초 단위
):
    """
    ✅ NOx 급등락(spike) 및 주변 구간 마킹 (불규칙 timestamp 대응, 시간 기준)
    - df를 직접 수정 (in-place), return 없음

    Parameters:
    - df : pd.DataFrame (원본에 컬럼 추가됨)
    - time_col : str (datetime 컬럼명)
    - nox_col : str (NOx 값 컬럼명)
    - window_time_sec : int (rolling 윈도우 길이, 초)
    - spike_range_threshold : float (급등락 range 기준)
    - spike_std_threshold : float (급등락 std 기준)
    - spike_window_sec : int (스파이크 주변 시간 범위, 초)
    """

    print("📈 급등락 피처 생성 중...")

    # ✅ 시간 기반 rolling window
    window_time = pd.Timedelta(seconds=window_time_sec)
    df["nox_range_1min"] = (
        df.rolling(window=window_time, on=time_col)[nox_col].max() -
        df.rolling(window=window_time, on=time_col)[nox_col].min()
    )
    df["nox_std_1min"] = df.rolling(window=window_time, on=time_col)[nox_col].std()

    # ✅ 급등락 여부
    df["is_spike"] = (
        (df["nox_range_1min"] > spike_range_threshold) &
        (df["nox_std_1min"] < spike_std_threshold)
    ).astype(int)

    spike_count = df["is_spike"].sum()
    print(f"   ✅ 급등락 구간 탐지: {spike_count}개 ({spike_count/len(df)*100:.2f}%)")

    # ✅ 스파이크 주변 과거/미래 마킹
    print("🎯 스파이크 주변 구간 마킹 중...")
    mask_past = pd.Series(False, index=df.index)
    mask_future = pd.Series(False, index=df.index)

    spike_times = df.loc[df["is_spike"] == 1, time_col]

    return None

mark_nox_spikes(df, time_col="_time_gateway", nox_col="nox_value")

"""# target(2.5분 뒤 NOx값) 및 target에 해당하는 weight 생성"""

def create_target_and_weights(
    df,
    time_col,
    nox_col,
    delta_sec=150
):
    print("🎯 타겟 변수 생성 중...")

    # 타겟 생성
    target_df = df[[time_col, nox_col]].copy()
    target_df[time_col] = target_df[time_col] - pd.Timedelta(seconds=delta_sec)
    target_df = target_df.rename(columns={nox_col: "target"})

    # ✅ merge 후 df 재할당
    df = pd.merge(df, target_df, on=time_col, how="left")
    print('')
    print(f"   ✅ 타겟 매핑 완료 (결측 타겟: {df['target'].isna().sum():,}개)")

    # 가중치 설정
    print("⚖️ 가중치 설정 중...")
    def custom_weight(value):
        if pd.isna(value):
            return 1.0
        elif value < 20:
            return 1.0
        elif value < 30:
            return 1.5
        elif value < 40:
            return 3.0
        elif value >= 60:
            return 3.0
        else:
            return 2.0

    df["weights"] = df["target"].apply(custom_weight)

    weight_stats = df["weights"].value_counts().sort_index()
    print("   가중치 분포:")
    for weight, count in weight_stats.items():
        print(f"     가중치 {weight}: {count:,}개 ({count/len(df)*100:.1f}%)")

    return df

df = create_target_and_weights(df, time_col="_time_gateway", nox_col="nox_value", delta_sec=150)

df.head()







"""# (필요 시 사용) 중간 정보 저장"""

parquet_path = os.path.join(dir_save, 'df.parquet')
meta_path = os.path.join(dir_save, 'df_category_meta.json')

import json

# === 1️⃣ category column 정보 수집 ===
cat_info = {}
for col in df.columns:
    if df[col].dtype.name == 'category':
        cat_info[col] = list(df[col].cat.categories.astype(str))

# === 2️⃣ DataFrame 저장 (category → str 변환) ===
df_for_save = df.apply(lambda x: x.astype(str) if x.dtype.name == 'category' else x)
df_for_save.to_parquet(parquet_path, index=False)

# === 3️⃣ category 메타데이터 저장 ===
with open(meta_path, 'w', encoding='utf-8') as f:
    json.dump(cat_info, f, ensure_ascii=False, indent=2)

print(f"✅ DataFrame 저장 완료: {parquet_path}")
print(f"✅ Category 정보 저장 완료: {meta_path}")

# # parquet 불러오기
# df = pd.read_parquet(parquet_path)

# # category 메타 불러오기
# with open(meta_path, 'r', encoding='utf-8') as f:
#     cat_info = json.load(f)

# # category 복원
# for col, cats in cat_info.items():
#     df[col] = pd.Categorical(df[col], categories=cats)

# print(df.dtypes)

### (임시) 데이터 중간부터 불러오면 사용 ###

# interval_seconds = [int(60*x) for x in [1, 3, 5, 10, 30]]
# cols_x_stat = []

# for col in cols_x_original:
#     for sec in interval_seconds:
#         cols_x_stat.extend([
#             f"{col}_mean_{sec}s",
#             f"{col}_std_{sec}s",
#             f"{col}_mean_rate_change_{sec}s",
#             f"{col}_range_change_{sec}s",
#             f"{col}_momentum_max_up_{sec}s",
#             f"{col}_momentum_max_down_{sec}s",
#             f"{col}_max_increase_from_start_{sec}s",
#             f"{col}_max_decrease_from_start_{sec}s"
#         ])







"""# 모델링용 데이터

## 내부 온도,출구 온도 범위 확인
"""

df.shape

# 1. 온도와 NOx 범위 설정 (50도, 10단위 구간)
temp_bins = list(range(550, 1250, 50))  # 예: [1100, 1150, 1200]
nox_bins  = list(range(  0,  130, 10))  # 예: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['icf_ccs_fg_t_1'], bins=temp_bins, edgecolor='black', color='skyblue')
plt.xticks(temp_bins)
plt.xlabel('내부 온도')
plt.ylabel('Count')
plt.title('내부 온도 Histogram')
plt.tight_layout()
plt.show()

# 2. 각 변수 bin으로 나누기
df['temp_bin'] = pd.cut(df['icf_ccs_fg_t_1'], bins=temp_bins, right=False)
df[ 'nox_bin'] = pd.cut(df[     'nox_value'], bins= nox_bins, right=False)

# 3. 2D 그룹별 카운팅
count_df = df.groupby(['nox_bin', 'temp_bin']).size().unstack(fill_value=0)
# 1. count_df의 행 순서를 뒤집기 (역순)
count_df_reversed = count_df.iloc[::-1]

# 시각화
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. 내부 온도')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

# 1. 온도와 NOx 범위 설정 (50도, 10단위 구간)
temp_bins = list(range(550, 1250, 50))  # 예: [1100, 1150, 1200]
nox_bins  = list(range(  0,  130, 10))  # 예: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['icf_scs_fg_t_1'], bins=temp_bins, edgecolor='black', color='skyblue')
plt.xticks(temp_bins)
plt.xlabel('출구 온도')
plt.ylabel('Count')
plt.title('출구 온도 Histogram')
plt.tight_layout()
plt.show()

# 2. 각 변수 bin으로 나누기
df['temp_bin'] = pd.cut(df['icf_scs_fg_t_1'], bins=temp_bins, right=False)
df[ 'nox_bin'] = pd.cut(df[     'nox_value'], bins= nox_bins, right=False)

# 3. 2D 그룹별 카운팅
count_df = df.groupby(['nox_bin', 'temp_bin']).size().unstack(fill_value=0)
# 1. count_df의 행 순서를 뒤집기 (역순)
count_df_reversed = count_df.iloc[::-1]

# 시각화
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. 출구 온도')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

# 1. O2와 NOx 범위 설정
o2_bins  = list(range(0,  22,  2))  # 예: [1100, 1150, 1200]
nox_bins = list(range(0, 130, 10))  # 예: [0, 10, ..., 120]

plt.figure(figsize=(7.5, 4))
plt.hist(df['br1_eo_o2_a'], bins=o2_bins, edgecolor='black', color='skyblue')
plt.xticks(o2_bins)
plt.xlabel('O2 (%)')
plt.ylabel('Count')
plt.title('보일러 O2 농도 Histogram')
plt.tight_layout()
plt.show()

# 2. 각 변수 bin으로 나누기
df[ 'o2_bin'] = pd.cut(df['br1_eo_o2_a'], bins= o2_bins, right=False)
df['nox_bin'] = pd.cut(df[  'nox_value'], bins=nox_bins, right=False)

# 3. 2D 그룹별 카운팅
count_df = df.groupby(['nox_bin', 'o2_bin']).size().unstack(fill_value=0)
# 1. count_df의 행 순서를 뒤집기 (역순)
count_df_reversed = count_df.iloc[::-1]

# 시각화
plt.figure(figsize=(10, 6))
sns.heatmap(count_df_reversed, annot=True, fmt='d', cmap='YlOrRd', vmax=200)
plt.title('NOx vs. 보일러 O2 농도')
plt.ylabel('')
plt.xlabel('')
# plt.tight_layout()
plt.show()

idx_modeling = (
    df[['icf_ccs_fg_t_1', 'icf_scs_fg_t_1']]
    .loc[df['icf_ccs_fg_t_1'] >= 850, ]
    .loc[df['icf_scs_fg_t_1'] >= 750, ]
    .index
)

df.shape[0] - len(idx_modeling)

"""## column 설정 + 온도 조건으로 row filtering"""

## 로그 추가

## target을 정확히 2.5분 뒤로 매핑 (index 말고 시간으로)
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time

# 파라미터
nox_col = "nox_value"
time_col = "_time_gateway"

# feature_cols = ["is_spike", "near_spike"] + cols_x_original + cols_x_stat
feature_cols = ["is_spike"] + cols_x_original + cols_x_stat

df_model = df.loc[idx_modeling, [time_col, nox_col, 'target', 'weights'] + feature_cols].sort_values(by=time_col).reset_index(drop=True)
print(f"   ✅ 기본 데이터 준비 완료 (행: {len(df_model):,}, 열: {len(df_model.columns)})")

"""## 결측치 처리
- 1. column 제외: column별 결측치 확인 후, target이 있는 row 중 결측치가 10,000개 이상이면 제외
- 2. row 제외: 1. 진행 후 결측 row 제외
"""

before_count = len(df_model)

# 컬럼별 결측치 개수 확인
na_count = df_model.dropna(subset=[col_nox_tms_af, 'target', 'weights'])[feature_cols].isna().sum()

# 내림차순 정렬
na_count_sorted = na_count.sort_values(ascending=False)

na_count_sorted.iloc[:50]

# 예시: na_count_sorted가 Series일 때
bins = np.arange(0, na_count_sorted.max() + 10000, 10000)  # 10000 단위 구간 생성
labels = [f"{int(bins[i])}~{int(bins[i+1])}" for i in range(len(bins)-1)]

# 구간별 개수 집계
na_count_bins = pd.cut(na_count_sorted, bins=bins, labels=labels, right=False)
na_bin_counts = na_count_bins.value_counts().sort_index()

na_bin_counts.loc[na_bin_counts > 0]

cols_na = [x for x in na_count_sorted.index[na_count_sorted > 10000].tolist() if x != 'target']

len(cols_na)

feature_cols = [x for x in feature_cols if x not in cols_na]
valid_idx = df_model[feature_cols + [col_nox_tms_af, "target", "weights"]].notna().all(axis=1)

valid_idx.value_counts(dropna=False)

df_model = df_model.loc[valid_idx].reset_index(drop=True)
after_count = len(df_model)
removed_count = before_count - after_count

print(f"   전체 행: {before_count:,} → {after_count:,}")
print(f"   제거된 행: {removed_count:,} ({removed_count/before_count*100:.1f}%)")

"""# LightGBM 모델링

## lightgbm library & GPU 설정

### (필요 시 사용) GPU 버전 lightgbm 설치
"""

# ### LGBM GPU 버전 ###

# # 1. 필수 패키지 설치
# !apt-get install -y -qq libboost-all-dev

# # 2. Git에서 LightGBM clone
# !git clone --recursive https://github.com/microsoft/LightGBM

# # 3. GPU 활성화한 상태로 빌드
# %cd LightGBM
# !mkdir build
# !cmake -DUSE_GPU=1 -DOpenCL_INCLUDE_DIR=/usr/include -DOpenCL_LIBRARY=/usr/lib/x86_64-linux-gnu/libOpenCL.so .
# !make -j4

# # 4. 파이썬 패키지 설치
# %cd python-package
# !python setup.py install

# # 5. Colab 작업 디렉토리로 돌아오기
# %cd /content

"""### import"""

!pip install LightGBM

# - lightgbm GPU버전 패키지 설치된 경로
# os.chdir('/content/drive/MyDrive/Colab Notebooks/baseline_modeling_250512_250704')

import lightgbm as lgb
print("LightGBM version:", lgb.__version__)
import subprocess

# === GPU 감지 함수 ===
def is_gpu_available():
    try:
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return result.returncode == 0
    except FileNotFoundError:
        return False

use_gpu = is_gpu_available()
print("GPU 사용 여부:", use_gpu)

"""## train, test 분할"""

# # Train/Test 분할
# print("\n✂️ 데이터 분할 중...")

# # 시간 순으로 정렬된 df_model에서 80%에 해당하는 시점을 찾기
# split_index = int(len(df_model) * 0.8)
# datetime_train_end = df_model.iloc[split_index][col_datetime]

# train_df = df_model.loc[df_model[col_datetime] <= datetime_train_end].copy()
# test_df  = df_model.loc[df_model[col_datetime] >  datetime_train_end].copy()

# print(f"   훈련 데이터: {len(train_df):,}개 ({len(train_df)/len(df_model)*100:.1f}%)")
# print(f"   테스트 데이터: {len(test_df):,}개 ({len(test_df)/len(df_model)*100:.1f}%)")

# print(f"\n   훈련 데이터 시작: {train_df[col_datetime].min()}")
# print(f"   훈련 데이터 종료: {train_df[col_datetime].max()}")
# print(f"   테스트 데이터 시작: {test_df[col_datetime].min()}")
# print(f"   테스트 데이터 종료: {test_df[col_datetime].max()}")

# Train/Test 분할 (사용자 지정 기간)
print("\n✂️ 데이터 분할 중 (사용자 지정 기간)...")

# 시간 순으로 정렬
df_model.sort_values(by=col_datetime, inplace=True)

# 지정된 학습 종료 시점
datetime_train_end = pd.to_datetime('2025-07-04 23:59:59')

# 지정된 테스트 시작/종료 시점
datetime_test_start = pd.to_datetime('2025-07-05')
datetime_test_end = pd.to_datetime('2025-07-16')

# 데이터 분할
train_df = df_model.loc[df_model[col_datetime] <= datetime_train_end].copy()
test_df  = df_model.loc[
    (df_model[col_datetime] >= datetime_test_start) &
    (df_model[col_datetime] <= datetime_test_end + pd.Timedelta(days=1, seconds=-1)) # Include the end of the last day
].copy()


print(f"   훈련 데이터: {len(train_df):,}개")
print(f"   테스트 데이터: {len(test_df):,}개")

print(f"\n   훈련 데이터 시작: {train_df[col_datetime].min()}")
print(f"   훈련 데이터 종료: {train_df[col_datetime].max()}")
print(f"   테스트 데이터 시작: {test_df[col_datetime].min()}")
print(f"   테스트 데이터 종료: {test_df[col_datetime].max()}")

"""## oversampling (training set)"""

# Oversampling
print("\n🔄 오버샘플링 수행 중...")
over_df = train_df[train_df["target"] >= 40]
over_sampled = over_df.sample(frac=1.0, replace=True, random_state=42)
original_train_size = len(train_df)
train_df = pd.concat([train_df, over_sampled])

print(f"   고농도 샘플(>=40ppm): {len(over_df):,}개")
print(f"   오버샘플링 후 훈련 데이터: {original_train_size:,} → {len(train_df):,}")











"""# 실험 3-0.

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

# model = lgb.LGBMRegressor(
#     random_state=42,
#     # verbose=-1,  # 학습 로그 숨김
#     n_estimators=1000,
#     learning_rate=0.03,
#     num_leaves=127,
#     max_depth=12,
#     min_child_samples=30,
#     reg_alpha=0.1,    # L1
#     reg_lambda=1.0,   # L2
#     device='gpu',
#     n_jobs=-1
# )


start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

"""## 결과 저장 - 모형, 변수 중요도"""

import pickle

# 모델 저장
with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
    pickle.dump(model, f)

import lightgbm as lgb
print(f"LightGBM version in current environment: {lgb.__version__}")

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""



# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")







"""# 실험 4. 2025년 4월 학습 데이터 추가

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

# Prepare test data for inference and save to CSV
print("\n💾 추론용 테스트 데이터 준비 및 저장 중...")

# Select relevant columns for inference: time, feature columns, and actual nox_value
cols_for_inference = [col_datetime, col_nox_tms_af] + feature_cols

# Ensure data is clean (should already be from previous steps, but double-check)
test_df_inference = test_df[cols_for_inference].dropna().copy()

# Define save path
inference_data_path = os.path.join(dir_save, "test_data_for_inference.csv")

# Save to CSV
test_df_inference.to_csv(inference_data_path, index=False)

print(f"   ✅ 추론용 테스트 데이터 저장 완료: {inference_data_path}")
print(f"   저장된 데이터 행: {len(test_df_inference):,}, 열: {len(test_df_inference.columns)}")

print(test_df_inference.columns)

test_df_inference.head(3)

# Prepare test data for inference and save to Parquet
print("\n💾 추론용 테스트 데이터 준비 및 Parquet 파일 저장 중...")

# Select relevant columns for inference: time, feature columns, and actual nox_value
# Assuming 'test_df_inference' is already prepared in the preceding cell
# cols_for_inference = [col_datetime, col_nox_tms_af] + feature_cols
# test_df_inference = test_df[cols_for_inference].dropna().copy()

# Define save path for Parquet
inference_data_path_parquet = os.path.join(dir_save, "test_data_for_inference.parquet")

# Save to Parquet
test_df_inference.to_parquet(inference_data_path_parquet, index=False)

print(f"   ✅ 추론용 테스트 데이터 저장 완료: {inference_data_path_parquet}")
print(f"   저장된 데이터 행: {len(test_df_inference):,}, 열: {len(test_df_inference.columns)}")

"""## 결과 저장 - 모형, 변수 중요도"""

# import pickle

# # 모델 저장
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")







"""# 실험 5번. 2025년 3월부터 학습 데이터 추가

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

feature_cols

"""## 결과 저장 - 모형, 변수 중요도"""

# import pickle

# # 모델 저장
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")







"""# 실험 6번. 2025년 5월 22일 부터 학습

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

"""## 결과 저장 - 모형, 변수 중요도"""

# import pickle

# # 모델 저장
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")





"""# 실험 7번. Baseline 모델에 신규 데이터 추가

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

"""## 결과 저장 - 모형, 변수 중요도"""

# import pickle

# # 모델 저장
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")





"""# 실험 8번. 5/22 부터의 모델에 신규 데이터 추가

## 모형 학습
"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
model = lgb.LGBMRegressor(
    random_state=42,
    n_estimators=100,
    device='gpu' if use_gpu else 'cpu'
)

start_time = time.time()
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

"""## 결과 저장 - 모형, 변수 중요도"""

# import pickle

# # 모델 저장
# with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
#     pickle.dump(model, f)

# # 변수 중요도 DataFrame 생성
# importance_df = pd.DataFrame({
#     "feature": feature_cols,
#     "importance": model.feature_importances_
# }).sort_values("importance", ascending=False)

# # CSV 저장
# importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# # pickle로 저장하고 싶다면
# with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
#     pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""### 그래프 및 결과 저장"""

# 성능 지표를 엑셀 파일로 저장
performance_metrics = {
    "MAE": mae,
    "RMSE": rmse
}

# 구간별 성능 데이터프레임에 MAE, RMSE, sMAPE 열 추가
# Corrected column name from 'sMAPE(%)' to 'sMAPE'
grouped_metrics = grouped.copy()


with pd.ExcelWriter(os.path.join(dir_save, "performance_metrics.xlsx")) as writer:
    pd.DataFrame([performance_metrics]).to_excel(writer, sheet_name="Overall_Metrics", index=False)
    grouped_metrics.to_excel(writer, sheet_name="Segment_Metrics", index=False)

print(f"✅ 성능 지표 저장 완료: {os.path.join(dir_save, 'performance_metrics.xlsx')}")

# 그래프 저장 (이전에 생성된 matplotlib figure 객체를 순회하며 저장)

print("\n💾 그래프 저장 중...")

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "feature_importance.png"), bbox_inches='tight')
plt.close()

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.savefig(os.path.join(dir_save, "timeseries_prediction.png"), bbox_inches='tight')
plt.close()


# 시각화 2: 잔차 분포 and 산점도
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "residuals_and_scatter.png"), bbox_inches='tight')
plt.close()


# 시각화 3: 구간별 성능 분석
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(dir_save, "segment_performance.png"), bbox_inches='tight')
plt.close()


print(f"✅ 그래프 저장 완료: {dir_save} 폴더")

















"""# 최종 -> 실험 3-1-2. 적용 + sample weight 변경"""

dir_save = os.path.join(
    '/content/drive/MyDrive/Colab Notebooks/250801_baseline_modeling_인수인계',
    '실험3_1_2_1'
)
os.makedirs(dir_save, exist_ok=True)

dir_save

"""## 모형 학습"""

# 모델 학습
print("\n🤖 LightGBM 모델 학습 중...")
# model = lgb.LGBMRegressor(
#     random_state=42,
#     n_estimators=100,
#     device='gpu' if use_gpu else 'cpu'
# )

model = lgb.LGBMRegressor(
    random_state=42,
    # verbose=-1,  # 학습 로그 숨김
    n_estimators=500,
    learning_rate=0.03,
    num_leaves=63,
    max_depth=10,
    min_child_samples=20,
    reg_alpha=0.1,    # L1
    reg_lambda=1.0,   # L2
    device='gpu',
    n_jobs=-1
)


start_time = time.time()
# model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"])
model.fit(train_df[feature_cols], train_df["target"], sample_weight=train_df["weights"].apply(lambda x: 2*x if x>=2 else x))
training_time = time.time() - start_time

print(f"   ✅ 모델 학습 완료! (소요시간: {training_time:.2f}초)")

# 예측
print("\n🔮 예측 수행 중...")
y_pred = pd.Series(model.predict(test_df[feature_cols]), index=test_df.index)
print("   ✅ 예측 완료!")

"""## 결과 저장 - 모형, 변수 중요도"""

import pickle

# 모델 저장
with open(os.path.join(dir_save, "lgbm_model.pkl"), "wb") as f:
    pickle.dump(model, f)

# 변수 중요도 DataFrame 생성
importance_df = pd.DataFrame({
    "feature": feature_cols,
    "importance": model.feature_importances_
}).sort_values("importance", ascending=False)

# CSV 저장
importance_df.to_csv(os.path.join(dir_save, "feature_importance.csv"), index=False)

# pickle로 저장하고 싶다면
with open(os.path.join(dir_save, "feature_importance.pkl"), "wb") as f:
    pickle.dump(importance_df, f)

"""## test set 성능 평가"""

# 평가
print("\n📊 모델 성능 평가...")
valid_idx = test_df["target"].notna() & y_pred.notna()
mae = mean_absolute_error(test_df["target"][valid_idx], y_pred[valid_idx])
rmse = np.sqrt(mean_squared_error(test_df["target"][valid_idx], y_pred[valid_idx]))

print("=" * 60)
print("🎉 최종 결과")
print("=" * 60)
print(f"✅ MAE (평균 절대 오차): {mae:.3f} ppm")
print(f"✅ RMSE (제곱근 평균 제곱 오차): {rmse:.3f} ppm")
print()

# 피처 중요도 확인
print("🏆 피처 중요도 Top 15:")
print("-" * 50)
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for i, row in importance_df.head(15).iterrows():
    print(f"{row['feature']:30} : {row['importance']:8.1f}")

# des 테이블에서 계측 항목 정보 가져오기
feature_desc_map = dict(zip(des['DATAFIELD_lowercased'], des['계측 항목 ']))

# 피처 중요도 시각화 (설명 포함)
plt.figure(figsize=(14, 10))
top_features = importance_df.head(20)

# y축 라벨 생성 (변수명 + 설명)
y_labels = []
for feature in top_features['feature']:
   # lag 정보 추출
   if '_lag' in feature:
       base_name = feature.split('_lag')[0]
       desc = feature_desc_map.get(base_name, '')
   else:
       desc = feature_desc_map.get(feature, '')

   if desc and pd.notna(desc):
       label = f"{feature}\n({desc})"
   else:
       label = feature
   y_labels.append(label)

plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), y_labels, fontsize=9)
plt.xlabel('Feature Importance', fontsize=12)
plt.title('Top 20 Feature Importance (피처 중요도)', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 그래프 간격 조정
plt.subplots_adjust(left=0.35)
plt.tight_layout()
plt.show()



# 결과 정리
result_df = test_df.copy()
result_df["NOx_실제값"] = test_df["target"]
result_df["NOx_예측값"] = y_pred

print("\n📈 시각화 생성 중...")

# 시각화 1: 시계열 예측 결과
plt.figure(figsize=(15, 6))
interval = 30  # 예: 100개 간격으로 샘플링
sample_result = result_df.iloc[::interval].copy()  # 일정 간격으로 샘플링
sample_result = sample_result.sort_values(time_col)  # 시간 기준 정렬 (선택적)

plt.plot(sample_result[time_col], sample_result["nox_value"],
         label="실제 NOx", alpha=0.7, linewidth=1.5)
plt.plot(sample_result[time_col], sample_result["NOx_예측값"],
         label="예측 NOx", alpha=0.7, linewidth=1.5)
plt.legend()
plt.title("2분 30초 뒤 NOx 예측 결과 (샘플)")
plt.xlabel("시간")
plt.ylabel("NOx (ppm)")
plt.grid(True, alpha=0.2)
plt.tight_layout()
plt.show()


# 시각화 2: 잔차 분포
residual = result_df["NOx_실제값"] - result_df["NOx_예측값"]
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.histplot(residual, bins=50, kde=True)
plt.title("예측 오차(잔차) 분포")
plt.xlabel("오차 (실제 - 예측)")
plt.grid(True, alpha=0.3)

# 산점도
plt.subplot(1, 2, 2)
plt.scatter(result_df["NOx_실제값"], result_df["NOx_예측값"], alpha=0.5)
plt.plot([result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         [result_df["NOx_실제값"].min(), result_df["NOx_실제값"].max()],
         'r--', linewidth=2)
plt.xlabel("실제 NOx")
plt.ylabel("예측 NOx")
plt.title("실제 vs 예측 산점도")
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 시각화 3: 구간별 성능 분석
print("📊 구간별 성능 분석 중...")
bins = np.arange(0, 120, 10)
result_df["NOx_bin"] = pd.cut(result_df["NOx_실제값"], bins=bins, right=False)
grouped = result_df.groupby("NOx_bin").agg(
    count=("NOx_실제값", "count"),
    ME=("NOx_실제값", lambda x: np.mean(x - result_df.loc[x.index, "NOx_예측값"])),
    MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - result_df.loc[x.index, "NOx_예측값"]))),
    RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - result_df.loc[x.index, "NOx_예측값"])**2))),
    sMAPE=("NOx_실제값", lambda x: np.mean(
        2 * np.abs(x - result_df.loc[x.index, "NOx_예측값"]) /
        (np.abs(x) + np.abs(result_df.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
    ) * 100),
    pos_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_count=("NOx_실제값", lambda x: np.sum((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
    pos_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) > 0)),
    neg_residual_ratio=("NOx_실제값", lambda x: np.mean((x - result_df.loc[x.index, "NOx_예측값"]) < 0)),
).reset_index()

print("\n📋 구간별 성능:")
print(grouped.to_string(index=False))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(grouped)), grouped["count"])
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 데이터 개수")
plt.xlabel("NOx 구간")
plt.ylabel("개수")
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(len(grouped)), grouped["MAE"], marker='o', label="MAE", linewidth=2)
plt.plot(range(len(grouped)), grouped["RMSE"], marker='s', label="RMSE", linewidth=2)
plt.xticks(range(len(grouped)), [str(x) for x in grouped["NOx_bin"]], rotation=45)
plt.title("NOx 구간별 예측 성능")
plt.xlabel("NOx 구간")
plt.ylabel("오차")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n🎊 모든 과정이 완료되었습니다!")
print(f"최종 모델 성능: MAE={mae:.3f}, RMSE={rmse:.3f}")

"""## test set 6시간 간격 그림"""

delta_sec = 150

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import OrderedDict

# 기본 폰트 크기 조절
plt.rcParams.update({
    'font.size': 10,
    'axes.labelsize': 11,
    'xtick.labelsize': 12,
    'ytick.labelsize': 13,
    'legend.fontsize': 11,
    'axes.titlesize': 13
})

# 6시간 구간 생성 함수
def get_6hr_interval_bounds(date):
    start = pd.Timestamp(date)
    return [(start + pd.Timedelta(hours=6 * i), start + pd.Timedelta(hours=6 * (i + 1))) for i in range(4)]

def plot_with_gap(ax, df, time_col, value_col, label=None, **plot_kwargs):
    df_sorted = df.sort_values(time_col).reset_index(drop=True)
    df_sorted["time_diff"] = df_sorted[time_col].diff().dt.total_seconds()
    segment_id = (df_sorted["time_diff"] > 5).cumsum()

    for _, segment in df_sorted.groupby(segment_id):
        ax.plot(segment[time_col], segment[value_col], label=label, **plot_kwargs)
        label = None  # 동일 라벨 반복 방지

# 날짜 컬럼 생성
result_df["date"] = result_df[time_col].dt.date
unique_dates = sorted(result_df["date"].unique())
result_df["time_target"] = result_df[time_col] + pd.Timedelta(seconds=delta_sec)

for date in unique_dates:
    intervals = get_6hr_interval_bounds(date)
    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(22, 11),
                            gridspec_kw={'width_ratios': [3.3, 1]},
                            constrained_layout=True)
    fig.subplots_adjust(hspace=0.2)

    for i, (start_time, end_time) in enumerate(intervals):
        mask = (result_df[time_col] >= start_time) & (result_df[time_col] < end_time)
        df_interval = result_df[mask].copy()

        ax_ts = axs[i][0]
        ax_table = axs[i][1]

        # 시계열 플롯
        # ax_ts.plot(df_interval[time_col], df_interval["NOx_실제값"], label="실제 NOx", alpha=0.7, linewidth=1.5)
        # ax_ts.plot(df_interval[time_col], df_interval["NOx_예측값"], label="예측 NOx", alpha=0.7, linewidth=1.5)
        # 시계열 플롯
        # plot_with_gap(ax_ts, df_interval, time_col, "NOx_실제값", label="실제 NOx", color='tab:blue',   alpha=0.7, linewidth=1.5)
        # ✅ 실제값은 time_target 기준으로 플롯
        plot_with_gap(ax_ts, df_interval, time_col, "nox_value",  label="실제 NOx", color='tab:blue',   alpha=0.7, linewidth=1.5)
        plot_with_gap(ax_ts, df_interval, time_col, "NOx_예측값", label="예측 NOx", color='tab:orange', alpha=0.7, linewidth=1.5)
        ax_ts.set_xlim([start_time, end_time])

        # ✅ y축: 최소 42.5까지 보장
        y_min, y_max = df_interval[["nox_value", "NOx_예측값"]].min().min(), df_interval[["nox_value", "NOx_예측값"]].max().max()
        ax_ts.set_ylim(bottom=0, top=max(42.5, y_max + 2))

        # ✅ 경계선 추가
        ax_ts.axhline(30, color="red", linestyle="--", linewidth=1)
        ax_ts.axhline(40, color="red", linestyle="-", linewidth=1)

        ax_ts.set_title(f"{start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%H:%M')}")
        ax_ts.set_ylabel("NOx (ppm)")
        ax_ts.grid(True, alpha=0.2)
        if i == 0:
            ax_ts.legend(loc='upper right')
        if i == 3:
            ax_ts.set_xlabel("시간")

        # x축 30분 간격 시간만 표시 (날짜 없음)
        ax_ts.xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0,30]))
        ax_ts.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        plt.setp(ax_ts.xaxis.get_majorticklabels(), rotation=0, ha='right')
        # ✅ 첫 번째 tick label 숨김
        xticklabels = ax_ts.get_xticklabels()
        if xticklabels:
            xticklabels[0].set_visible(False)

        # y축 tick label 0과 x축 tick 겹침 방지 (y축 레이블 위로 약간 이동)
        ax_ts.yaxis.set_tick_params(pad=10)

        # x축 첫 tick 약간 오른쪽으로 이동(간격이 5초 이상이면 선 끊김 반영)
        ticks = ax_ts.get_xticks()
        if len(ticks) > 1:
            ticks = np.array(ticks)
            ticks[0] += (ticks[1] - ticks[0]) * 0.05
            ax_ts.set_xticks(ticks)

        # 테이블 생성
        ax_table.axis("off")
        if not df_interval.empty:
            bins = np.arange(0, 120, 10)
            df_interval["NOx_bin"] = pd.cut(df_interval["NOx_실제값"], bins=bins, right=False)

            grouped = df_interval.groupby("NOx_bin").agg(
                count=("NOx_실제값", "count"),
                ME=("NOx_실제값", lambda x: np.mean(x - df_interval.loc[x.index, "NOx_예측값"])),
                MAE=("NOx_실제값", lambda x: np.mean(np.abs(x - df_interval.loc[x.index, "NOx_예측값"]))),
                RMSE=("NOx_실제값", lambda x: np.sqrt(np.mean((x - df_interval.loc[x.index, "NOx_예측값"])**2))),
                sMAPE=("NOx_실제값", lambda x: np.mean(
                    2 * np.abs(x - df_interval.loc[x.index, "NOx_예측값"]) /
                    (np.abs(x) + np.abs(df_interval.loc[x.index, "NOx_예측값"]).replace(0, np.nan))
                ) * 100),
            ).reset_index()

            # 문자열 bin 범위
            grouped["NOx_bin"] = grouped["NOx_bin"].astype(str)
            numeric_cols = ["count", "ME", "MAE", "RMSE", "sMAPE(%)"]
            grouped.rename(columns={"sMAPE": "sMAPE(%)"}, inplace=True)
            rounded = grouped[numeric_cols].round(2)

            # 결측치 또는 count==0인 셀을 "-"로 표시
            display_data = pd.concat([grouped[["NOx_bin"]], rounded], axis=1).astype(object)
            for col in numeric_cols:
                display_data.loc[display_data["count"] == 0, col] = "-"
                display_data[col] = display_data[col].replace([np.nan, np.inf, -np.inf], "-")

            # 테이블 그리기
            table = ax_table.table(
                cellText=display_data.values.tolist(),
                colLabels=display_data.columns,
                cellLoc="center",
                loc="center"
            )
            table.auto_set_font_size(False)
            table.set_fontsize(10.5)
            table.scale(1, 1.05)

    # 저장
    save_path = os.path.join(dir_save, f"NOx_prediction_6hr_{date}.png")
    plt.tight_layout()
    # 저장 (tight 옵션 유지)
    plt.savefig(save_path, dpi=300)  # bbox_inches='tight'는 constrained_layout이면 불필요
    plt.close()

dir_save





df.head(3)

"""## 4월 한달간의 요소수 주입량"""

import matplotlib.dates as mdates

# Filter data for the specified date range
start_date = pd.to_datetime('2025-04-06')
end_date = pd.to_datetime('2025-04-30')

df_filtered = df[
    (df[col_datetime] >= start_date) &
    (df[col_datetime] <= end_date + pd.Timedelta(days=1, seconds=-1)) # Include the end of the last day
].copy()

# Sample every 10th data point
df_sampled = df_filtered.iloc[::10].copy()

# Get unique dates in the filtered data
unique_dates = df_sampled[col_datetime].dt.date.unique()

# Create subplots for each date
n_dates = len(unique_dates)
fig, axes = plt.subplots(nrows=n_dates, figsize=(15, 4 * n_dates), sharex=False)

# Ensure axes is an array even if there's only one subplot
if n_dates == 1:
    axes = [axes]

for i, date in enumerate(unique_dates):
    ax = axes[i]
    df_date = df_sampled[df_sampled[col_datetime].dt.date == date]

    ax.plot(df_date[col_datetime], df_date[col_pump_hz])
    ax.set_title(f"Urea Pump Hz - {date}")
    ax.set_ylabel("Hz")
    ax.grid(True, alpha=0.3)

    # Format x-axis to show time
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
    fig.autofmt_xdate() # Auto-rotate date labels

plt.tight_layout()
plt.show()

"""# 최종 데이터 저장

## feature 외 기본 정보 column 설정
"""

y_pred_all = pd.Series(model.predict(df_model[feature_cols]), index=df_model.index)

df_model['nox_pred_future'] = y_pred_all

df_model['train_test_etc'] = df_model[col_datetime].apply(lambda x: 'train' if x <= datetime_train_end else 'test')
df_model['is_row_for_modeling'] = True

cols_basic = [

    # 시각
    col_datetime

    # - 모델링 대상 여부
    ,'is_row_for_modeling'
    # - 학습 대상 여부
    ,'train_test_etc'

    # 현재(t)시점 NOx 실제값
    ,'nox_value'
    # 현재(t)시점까지 feature를 사용한, 2.5분 뒤 NOx 예측값
    ,'nox_pred_future'
    # 2.5분 뒤 미래 시점의 NOx
    # - 실제값
    ,'nox_value_future'

    ### feature로 사용하진 않았지만, spike 및 weight 정의 시 사용된 부가 정보 ###
    ,'nox_range_1min'
    ,'nox_std_1min'
    ,'weights'
]

"""## 모델링 포함 데이터 + 제외 데이터"""

df_final = pd.merge(
    df.rename(columns={'target': 'nox_value_future'}),
    df_model[[col_datetime, 'nox_pred_future', 'train_test_etc', 'is_row_for_modeling']],
    how='left',
    on=col_datetime
)

df_final = df_final[[x for x in cols_basic] + [x for x in df_final.columns if x not in cols_basic]]

df_final.sort_values(by=col_datetime, inplace=True)
df_final.reset_index(drop=True, inplace=True)

df_final['is_row_for_modeling'] = df_final['is_row_for_modeling'].fillna(False)
df_final[     'train_test_etc'] = df_final[     'train_test_etc'].fillna('etc')

"""## NOx 1분 전 diff"""

# Datetime → index
df_final = df_final.sort_values(col_datetime).set_index(col_datetime)

# 1분 전의 시각 생성
target_time = df_final.index - pd.Timedelta(minutes=1)

# dict 매핑
nox_bf_map = df_final[col_nox_tms_bf].to_dict()
nox_af_map = df_final[col_nox_tms_af].to_dict()

# 1분 전 값 찾기
nox_bf_1min_ago = target_time.map(nox_bf_map)
nox_af_1min_ago = target_time.map(nox_af_map)

# 새로운 컬럼 2개 생성
df_final.insert(7, 'nox_diff_bf_1min', df_final[col_nox_tms_bf] - nox_bf_1min_ago.values)
df_final.insert(8, 'nox_diff_af_1min', df_final[col_nox_tms_af] - nox_af_1min_ago.values)

# index 복원
df_final.reset_index(drop=False, inplace=True)

"""## 저장"""

parquet_path = os.path.join(dir_save, 'df_final.parquet')
meta_path = os.path.join(dir_save, 'df_final_category_meta.json')

import json

# === 1️⃣ category column 정보 수집 ===
cat_info = {}
for col in df_final.columns:
    if df_final[col].dtype.name == 'category':
        cat_info[col] = list(df_final[col].cat.categories.astype(str))

# === 2️⃣ DataFrame 저장 (category → str 변환) ===
df_final_for_save = df_final.apply(lambda x: x.astype(str) if x.dtype.name == 'category' else x)
df_final_for_save.to_parquet(parquet_path, index=False)

# === 3️⃣ category 메타데이터 저장 ===
with open(meta_path, 'w', encoding='utf-8') as f:
    json.dump(cat_info, f, ensure_ascii=False, indent=2)

print(f"✅ DataFrame 저장 완료: {parquet_path}")
print(f"✅ Category 정보 저장 완료: {meta_path}")

df_final



