import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)


class NOxDataPreprocessor:
    """NOx ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.feature_cols = []
        self.logger = logging.getLogger(__name__)

    def preprocess_realtime_data(self, raw_data):
        """ì‹¤ì‹œê°„ ë°ì´í„° ì „ì²˜ë¦¬ í†µí•© íŒŒì´í”„ë¼ì¸"""
        self.logger.info("ğŸš€ NOx ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

        # 1ë‹¨ê³„: ê¸°ë³¸ ì „ì²˜ë¦¬
        data = self._basic_preprocessing(raw_data.copy())

        # 2ë‹¨ê³„: íê¸°ë¬¼ íˆ¬ì… í”¼ì²˜
        data = self._create_trash_drop_features(data)

        # 3ë‹¨ê³„: ìš”ì•½í†µê³„ëŸ‰ í”¼ì²˜
        data, cols_x_stat = self._generate_interval_summary_features(data)

        # 4ë‹¨ê³„: íŠ¹ìˆ˜ í”¼ì²˜
        data = self._mark_nox_spikes(data)

        # 5ë‹¨ê³„: ìµœì¢… í”¼ì²˜ ëª©ë¡ ìƒì„±
        self.feature_cols = self._create_final_feature_list(data, cols_x_stat)

        # 6ë‹¨ê³„: ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        model_data = self._prepare_model_input(data)

        self.logger.info("ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        return model_data, self.feature_cols

    def _basic_preprocessing(self, data):
        """ê¸°ë³¸ ë°ì´í„° ì •ë¦¬"""
        self.logger.info("1ï¸âƒ£ ê¸°ë³¸ ì „ì²˜ë¦¬")

        if "_time_gateway" in data.columns:
            data["_time_gateway"] = pd.to_datetime(data["_time_gateway"])
            # ì‹œê°„ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            data = data.set_index("_time_gateway")
            self.logger.info("   âœ… ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜ ë° ì¸ë±ìŠ¤ ì„¤ì • ì™„ë£Œ")

        self.logger.info(f"   ë°ì´í„° í˜•íƒœ: {data.shape}")
        return data

    def _create_trash_drop_features(self, data):
        """íê¸°ë¬¼ íˆ¬ì… í”¼ì²˜ ìƒì„±"""
        self.logger.info("2ï¸âƒ£ íê¸°ë¬¼ íˆ¬ì… í”¼ì²˜ ìƒì„±")

        if "icf_cra_wt_k" not in data.columns:
            self.logger.warning(
                "   âš ï¸ icf_cra_wt_k ì»¬ëŸ¼ì´ ì—†ì–´ íê¸°ë¬¼ íˆ¬ì… í”¼ì²˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
            )
            data["trash_drop"] = 0
            data["trash_drop_count_30min"] = 0
            return data

        window_size_sec = 10
        diff_tolerance = -10

        data["trash_drop"] = (
            data["icf_cra_wt_k"].bfill().rolling(window_size_sec).max().diff()
            < diff_tolerance
        ).astype(int)

        data["trash_drop_count_30min"] = (
            data["trash_drop"].rolling("30min").sum().fillna(0)
        )

        self.logger.info("   âœ… íê¸°ë¬¼ íˆ¬ì… í”¼ì²˜ ìƒì„± ì™„ë£Œ")
        return data

    def _generate_interval_summary_features(self, data):
        """ìš”ì•½í†µê³„ëŸ‰ í”¼ì²˜ ìƒì„±"""
        self.logger.info("3ï¸âƒ£ ìš”ì•½í†µê³„ëŸ‰ í”¼ì²˜ ìƒì„±")

        # ì›ë³¸ ë³€ìˆ˜ ëª©ë¡
        cols_x_original = [
            "bft_eo_fg_t",
            "br1_eo_fg_t",
            "br1_eo_o2_a",
            "br1_eo_st_t",
            "dr1_eq_bw_c",
            "icf_ccs_fg_t_1",
            "icf_cra_wt_k",
            "icf_ff1_ar_f_1",
            "icf_ff1_ss_s_1",
            "icf_ff1_ss_s_2",
            "icf_ff2_ss_s_1",
            "icf_idf_ss_s_1",
            "icf_scs_fg_t_1",
            "icf_tms_nox_a",
            "sdr_htr_fg_t",
            "trash_drop",
            "trash_drop_count_30min",
        ]

        interval_seconds = [60, 180, 300, 600, 1800]  # 1ë¶„, 3ë¶„, 5ë¶„, 10ë¶„, 30ë¶„
        new_columns = []

        for col in cols_x_original:
            if col in data.columns:
                self.logger.info(f"   ğŸ”„ {col} ì²˜ë¦¬ ì¤‘...")
                for sec in interval_seconds:
                    window = pd.Timedelta(seconds=sec)

                    # í‰ê· /í‘œì¤€í¸ì°¨
                    mean_col = f"{col}_mean_{sec}s"
                    std_col = f"{col}_std_{sec}s"
                    data[mean_col] = data.rolling(window=window)[col].mean()
                    data[std_col] = data.rolling(window=window)[col].std()
                    new_columns.extend([mean_col, std_col])

                    # ë³€í™”ìœ¨/ë³€í™”ëŸ‰
                    data_prev = data[[col]].copy()
                    data_prev_shifted = data_prev.copy()
                    data_prev_shifted.index += pd.Timedelta(seconds=sec)
                    data_prev_shifted.rename(columns={col: "_start_tmp"}, inplace=True)

                    data_matched = pd.merge_asof(
                        data,
                        data_prev_shifted,
                        left_index=True,
                        right_index=True,
                        direction="backward",
                        tolerance=pd.Timedelta(seconds=0),
                    )

                    start_val = data_matched["_start_tmp"].values
                    end_val = data[col].values

                    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                    start_val_safe = np.where(start_val == 0, 1e-10, start_val)

                    mean_rate_col = f"{col}_mean_rate_change_{sec}s"
                    range_change_col = f"{col}_range_change_{sec}s"
                    data[mean_rate_col] = (end_val - start_val) / start_val_safe
                    data[range_change_col] = end_val - start_val
                    new_columns.extend([mean_rate_col, range_change_col])

                    # ëª¨ë©˜í…€
                    data["_val_diff"] = data[col].diff()
                    data["_time_diff"] = (
                        data.index.to_series().diff().dt.total_seconds()
                    )
                    data["_rate_per_sec"] = data["_val_diff"] / data[
                        "_time_diff"
                    ].replace(0, 1e-10)

                    momentum_up_col = f"{col}_momentum_max_up_{sec}s"
                    momentum_down_col = f"{col}_momentum_max_down_{sec}s"
                    data[momentum_up_col] = data.rolling(window=window)[
                        "_rate_per_sec"
                    ].max()
                    data[momentum_down_col] = data.rolling(window=window)[
                        "_rate_per_sec"
                    ].min()
                    new_columns.extend([momentum_up_col, momentum_down_col])

                    # ì‹œì‘ê°’ ëŒ€ë¹„ ìµœëŒ€ ì¦ê°€/ê°ì†ŒëŸ‰
                    rolling_window = data.rolling(window=window)[col]
                    max_inc_col = f"{col}_max_increase_from_start_{sec}s"
                    max_dec_col = f"{col}_max_decrease_from_start_{sec}s"
                    data[max_inc_col] = rolling_window.max() - start_val
                    data[max_dec_col] = rolling_window.min() - start_val
                    new_columns.extend([max_inc_col, max_dec_col])
            else:
                self.logger.warning(f"   âš ï¸ {col} ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")

        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        data.drop(
            columns=["_val_diff", "_time_diff", "_rate_per_sec"],
            inplace=True,
            errors="ignore",
        )

        self.logger.info(
            f"   âœ… ìš”ì•½í†µê³„ëŸ‰ í”¼ì²˜ ìƒì„± ì™„ë£Œ - {len(new_columns)}ê°œ ì»¬ëŸ¼ ì¶”ê°€"
        )
        return data, new_columns

    def _mark_nox_spikes(self, data):
        """NOx ê¸‰ë“±ë½ í”¼ì²˜ ìƒì„±"""
        self.logger.info("4ï¸âƒ£ NOx ê¸‰ë“±ë½ í”¼ì²˜ ìƒì„±")

        if "nox_value" not in data.columns:
            self.logger.warning(
                "   âš ï¸ nox_value ì»¬ëŸ¼ì´ ì—†ì–´ NOx ê¸‰ë“±ë½ í”¼ì²˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
            )
            data["nox_range_1min"] = 0
            data["nox_std_1min"] = 0
            data["is_spike"] = 0
            return data

        window_time_sec = 60
        spike_range_threshold = 8
        spike_std_threshold = 6

        window_time = pd.Timedelta(seconds=window_time_sec)
        data["nox_range_1min"] = (
            data.rolling(window=window_time)["nox_value"].max()
            - data.rolling(window=window_time)["nox_value"].min()
        )
        data["nox_std_1min"] = data.rolling(window=window_time)["nox_value"].std()

        data["is_spike"] = (
            (data["nox_range_1min"] > spike_range_threshold)
            & (data["nox_std_1min"] < spike_std_threshold)
        ).astype(int)

        spike_count = data["is_spike"].sum()
        self.logger.info(
            f"   âœ… ê¸‰ë“±ë½ êµ¬ê°„ íƒì§€: {spike_count}ê°œ ({spike_count/len(data)*100:.2f}%)"
        )

        return data

    def _create_final_feature_list(self, data, cols_x_stat):
        """ìµœì¢… í”¼ì²˜ ëª©ë¡ ìƒì„±"""
        self.logger.info("5ï¸âƒ£ ìµœì¢… í”¼ì²˜ ëª©ë¡ ìƒì„±")

        cols_x_original = [
            "bft_eo_fg_t",
            "br1_eo_fg_t",
            "br1_eo_o2_a",
            "br1_eo_st_t",
            "dr1_eq_bw_c",
            "icf_ccs_fg_t_1",
            "icf_cra_wt_k",
            "icf_ff1_ar_f_1",
            "icf_ff1_ss_s_1",
            "icf_ff1_ss_s_2",
            "icf_ff2_ss_s_1",
            "icf_idf_ss_s_1",
            "icf_scs_fg_t_1",
            "icf_tms_nox_a",
            "sdr_htr_fg_t",
            "trash_drop",
            "trash_drop_count_30min",
        ]

        feature_cols = ["is_spike"] + cols_x_original + cols_x_stat

        # ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ ì œê±°
        na_count = data[feature_cols].isna().sum()
        cols_to_remove = na_count[na_count > 10000].index.tolist()
        feature_cols = [col for col in feature_cols if col not in cols_to_remove]

        self.logger.info(f"   âœ… ìµœì¢… í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ")
        if cols_to_remove:
            self.logger.info(f"   ğŸ—‘ï¸ ì œê±°ëœ í”¼ì²˜: {len(cols_to_remove)}ê°œ")

        return feature_cols

    def _prepare_model_input(self, data):
        """ëª¨ë¸ ì…ë ¥ ì¤€ë¹„"""
        self.logger.info("6ï¸âƒ£ ëª¨ë¸ ì…ë ¥ ì¤€ë¹„")

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        model_input_cols = self.feature_cols
        model_data = data[model_input_cols].copy()

        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°
        before_count = len(model_data)
        model_data = model_data.fillna(0)  # dropna() ëŒ€ì‹  fillna(0) ì‚¬ìš©
        after_count = len(model_data)

        self.logger.info(f"   ğŸ“Š ë°ì´í„° ì •ë¦¬: {before_count:,} â†’ {after_count:,} í–‰")
        self.logger.info(f"   ğŸ¯ ìµœì¢… ë°ì´í„° í˜•íƒœ: {model_data.shape}")

        return model_data
